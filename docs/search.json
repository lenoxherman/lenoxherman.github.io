[
  {
    "objectID": "Working Blogs/Untitled-1.html",
    "href": "Working Blogs/Untitled-1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\nfrom matplotlib import pyplot as plt\n\n# Generate regression data\ndef regression_data(n=100, w=torch.Tensor([-0.7, 0.5]), x_max=1):\n    x = torch.rand(n) * x_max\n    y = x * w[1] + w[0] + 0.05 * torch.rand(n)\n    return x, y\n\n# Define the empirical risk (mean squared error)\ndef empirical_risk(x, y, w, b):\n    y_pred = w * x + b\n    return torch.mean((y_pred - y)**2)\n\n# Stochastic Gradient Descent (SGD) algorithm\ndef sgd_algorithm(x, y, w_init=torch.rand(2), lr=0.1, epochs=100):\n    w = torch.tensor(w_init.clone(), requires_grad=True)\n    b = torch.tensor(torch.rand(1), requires_grad=True)\n    \n    empirical_risks = []\n    \n    for epoch in range(epochs):\n        # Shuffle indices for each epoch\n        indices = torch.randperm(x.shape[0])\n        \n        for idx in indices:\n            x_i = x[idx]\n            y_i = y[idx]\n            \n            # Compute gradients of the empirical risk (MSE) w.r.t. w and b\n            loss = (w * x_i + b - y_i)**2\n            loss.backward()\n            \n            # Update parameters using gradients\n            with torch.no_grad():\n                w -= lr * w.grad\n                b -= lr * b.grad\n                \n                # Manually zero the gradients after updating\n                w.grad.zero_()\n                b.grad.zero_()\n        \n        # Compute and store empirical risk at the end of each epoch\n        loss_epoch = empirical_risk(x, y, w, b)\n        empirical_risks.append(loss_epoch.item())\n    \n    return w, b, empirical_risks\n\n# Generate data\nx, y = regression_data()\n\n# Run SGD algorithm\nw_hat, b_hat, empirical_risks = sgd_algorithm(x, y)\n\n# Plot empirical risk evolution during a single epoch\nplt.figure(fig_size=(10, 5))\nplt.plot(empirical_risks, label='Empirical Risk')\nplt.xlabel('Iterations')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution During a Single Epoch')\nplt.legend()\nplt.show()\n\n# Plot empirical risk evolution over 100 epochs\n_, _, empirical_risks_over_epochs = sgd_algorithm(x, y, epochs=100)\n\nplt.figure(fig_size=(10, 5))\nplt.plot(empirical_risks_over_epochs, label='Empirical Risk')\nplt.xlabel('Epochs')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution Over 100 Epochs')\nplt.legend()\nplt.show()\n\n/var/folders/10/9wfcg10s3yz16fgwcvljkq340000gn/T/ipykernel_7636/925773991.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  w = torch.tensor(w_init.clone(), requires_grad=True)\n/var/folders/10/9wfcg10s3yz16fgwcvljkq340000gn/T/ipykernel_7636/925773991.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  b = torch.tensor(torch.rand(1), requires_grad=True)\n\n\nRuntimeError: grad can be implicitly created only for scalar outputs"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Blog3/WomeninDataScience.html",
    "href": "posts/Blog3/WomeninDataScience.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "WOMEN IN DATA SCIENCE BLOG POST\nDespite significant advancements in feminist movements, women remain conspicuously underrepresented in fields such as computing, mathematics, and engineering, which not only affects those within the industry but also the industry itself. Diversity fosters informed decision-making and sparks creativity. However, the current representation of women in STEM careers mirrors that of the 1960s, despite increased opportunities and educational access. This disparity, particularly in engineering, stems from historical shifts in the culture surrounding computing and subsequent academic requirements that disadvantage women and marginalized groups. To address this, efforts spotlighting the achievements of women in STEM are crucial. The Women in Data Science Conference that I attended, did just this. Highlighting women working with data in a variety of fields, such as political science, geography, engineering, and computer science the conference made space to discuss the different avenues women have taken with working with data. Additionally, the conference featured an alumni panel to give insight to Middlebury female alums now working in STEM fields. I really enjoyed this section as there is always a gap between education in college and success in the career world, and I enjoyed talking to alums, both recent and no0t so recent about how they used data science to make that jump.\nEven though we got the opportunity to speak with women in data related careers, these industries overall lack female representation. When industries lack proper diversity, perspectives become skewed, hindering unbiased innovation “The United States simply can’t afford to ignore the perspectives of half the population in future engineering and technical designs” (3. Corbett and Hill, 2015). Without diversity and representation in STEM industries, “everyone misses out on the novel solutions that diverse participation brings”(8, Corbett and Hill, 2015). Sometimes for non-diverse groups, the issue isn’t generating the “wrong” or unoptimal solution, its misidentifying the problem altogether.\nRight now, 26% of the population in STEM careers are women. This is an identical representation to STEM as the 1960s, despite the increased opportunity and education of women in STEM. Engineering is the most sex-segregated profession; however, compared to other profession such as lawyers or doctors, the gap between male engineers and female is exponentially wider. In the 1960s, engineering was a new field in need of workers, so it attracted men and women alike. However, when the personal computer was developed in the 80s, it created a hobbyist culture around computers, creating a gamer subculture that became dominated by men. Given the increasingly prevalence of computers both in the home and at work, colleges added increasingly stringent requirements for the computer science major. These additional requirements disproportionately disadvantaged women and people of other marginalized groups who were entering college with less math and programming experience.\nBreaking down barriers and challenging the underrepresentation of women in these fields, can be done by events that spotlighting the achievements of women in STEM, like this panel. Its important to combat workplace hostility towards women, as they are leaving STEM based jobs at a much higher rate than men. By providing visible role models, challenging stereotypes, and building support networks, these events inspire and empower women to pursue careers in STEM. They also advocate for change by raising awareness of systemic issues such as gender bias in hiring and promotion processes. By inspiring future generations of girls and students, these events help to create a more inclusive and diverse STEM community, ultimately contributing to greater gender equality in these fields.\nThis need for diversity within disciplines, extends beyond the STEM communities. Just as the STEM industry grapples with the challenge of inclusively, Amy Yuen’s examination of the Security Council sheds light on the complexities of achieving equal representation in influential decision-making bodies, like the United Nations. These insights show the importance of addressing systemic barriers and promoting diversity to create more inclusive environments conducive to progress and equitable outcomes. Amy Yuen is political science professor, who shared with us how she utilized data and data modeling to determine if the UN security council was “democratic” and offered equal representation. She first shared some necessary background information about the council, describing how there are 5 core countries with veto power who do not move off the council, but 10 other seats in which countries campaign for based on region. Despite the obvious lack of democratic voting within the council, Yuen’s focus was on evaluating the issue of equal representation. Each country holds a rotating elected seat for a month, and Yuen measured success in terms of their output during this tenure. Interestingly, she discovered that factors such as a nation’s wealth or political policies did not significantly impact their monthly output. Instead, she found that sponsorship played a pivotal role in determining the level of output on the council. Despite some countries like Japan or Brazil frequently serving on the council, Yuen noted close to equal representation among rotating elected members.\nSimilarly, Sarah M Brown, our keynote speaker, took a broader approach in investigating equal representation. She came all the way up from the University of Rhode Island to talk to us about the ethics of machine learning and how we can create systems that accurately represent society’s composition. As we are using machine learning more and more, specifically developing machine learning for sociotechnical systems, we need to direct our focus to making machine learning more fair and evaluate the context in which we create these systems.\nHer presentation started off by defining data science as equal parts computer science, stats, and domain expertise. Contrary to common practice, she emphasized the criticality of integrating domain expertise from the outset, rather than merely consulting domain experts later in the process. Contextualizing data, she argued, is key to unlocking solutions to STEM-related challenges. Expounding on this, she presented three fundamental “keys” that have shaped her work. However, she argued that it is important to contextualize this data, and pull upon non computer science or math backgrounds to unlock the keys to solving STEM related issues. The rest of the presentation was an exploration of three “keys” that have helped to unlock her work.\nHer first key was introduced back in 2004 in her high school social studies course. Her teacher described the need to use context to decipher primary sources. Drawing parallels to data as primary sources, Brown stressed the necessity of contextual understanding in deciphering, cleaning, and implementing data effectively. Her second key highlighted the social nuances and disciplinary norms inherent in different fields. She emphasized the need to situate data and models within the framework of the relevant discipline to ensure accurate interpretation and application. As a psych major, I particularly enjoyed this part of the talk. My interest is where the intersect of these two disciplines lie. My “domain skills” are being able to connect people and social leadership with data models and computer science, which can be isolating at times. The last “key” she mentioned was to meet people where they are. She recognized the power of precedent in shaping attitudes towards innovation and change. Using examples, she illustrated the challenges in prioritizing fairness over accuracy, urging for creative solutions rather than mere mathematical refinement.\nShe briefly introduced the concept of “algorithmic reparations,” which is something I had never heard before. Brown provocatively challenged the prevailing notion that accuracy must always take precedence over fairness, highlighting the need for a shift in perspective within the data science community. From a psychological standpoint, she emphasized the imperative of creative thinking in addressing these ethical dilemmas, signaling a departure from traditional approaches grounded solely in mathematical optimization. She ended the lecture on a positive note, that while we are creating machine learning systems faster than we are auditing them, society is becoming increasingly vigilant in privatizing non-biased systems.\nDr. Jessica L’roe, a female professor at Middlebury, continued to emphasize the importance of context within research like Professor Brown, drawing from her own study on deforestation and afforestation in various regions of Africa and their impacts on local communities. Employing a multifaceted approach, she collected both qualitative and quantitative data, enriching numerical findings with narratives from the local populace. One notable observation she shared was the rising population of smallholder farmers around a national park in Uganda, coinciding with an increase in tree planting activities. Looking deeper into the dynamics, she uncovered that non-local landowners were primarily responsible for the tree planting initiatives, often planting non-native species. Another intriguing aspect explored was the intergenerational changes in land parcel sizes and their implications on local livelihoods. Mothers in the area were interviewed and confessed that they were prioritizing education over agricultures for the children, because they feared that there would not be available land to farm. This nuanced examination highlights the significance of considering social dynamics alongside environmental changes and data in understanding complex issues like deforestation and community resilience.\nLaura Biester was our last keynote speaker and offered a different perspective, focusing more on training models to a data set. She presented on her research on computational linguistic models of mental health. Shifting the focus towards training models with specific datasets derived from public internet forums like Twitter and Reddit, Biester explored the utilization of language, particularly the use of first-person pronouns, as a measure of decreased mental health, among individuals with depression. However, Biester also addressed the challenges associated with accessing high-quality data due to privacy concerns, emphasizing the need for more representative datasets. I had never heard of natural language processing, so I found her talk very interesting. because it also once again combined my interests of psychology and computer science. Her model discovered that natural language models do a better job of capturing emotions, and manifested depressive symptoms, while linear regression models focus more on discussion of overall mental health and medication. Through her investigation, she uncovered the potential of out-of-domain datasets to test the generalizability of trained models, paving the way for a more holistic understanding of depression, emotions, and relationships through language processing techniques.\nIn conclusion, achieving gender equality and diversity in STEM fields requires concerted efforts to challenge systemic biases and provide support for underrepresented groups. By recognizing the value of diversity and promoting inclusive representation, industries can foster innovation and creativity while ensuring equitable opportunities for all. The insights shared by various speakers, spanning disciplines from political science to linguistics, to geography highlight the multidimensional nature of the issue and all the ways that data can be explored and utilized towards a passion. Moving forward, I learned that continued education, contextualizing data and systemic reform are essential to create a more inclusive and equitable future for women both in and out of STEM professions."
  },
  {
    "objectID": "posts/Blog1/BlogPost1.html",
    "href": "posts/Blog1/BlogPost1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nSetting up cross valadation score, establishing my classifier as a random object with no paramenters. CV is the amount of time we cross-validate and then return our mean scores\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n“cols” was just for visualizing how to combine column, the training and test columns will be run on the variable “predicted colmns which includes the columns we haven’t dropped, culmen length and culmen depth. Cols above is compare all the data points across species inorder to produce the pair plot below\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguin.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove we prepared the data preparing the data and then preforming a train_test spilt\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/Blog1/BlogPost1revised.html",
    "href": "posts/Blog1/BlogPost1revised.html",
    "title": "Classifying Palmer Penguin Species",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nClassifying Palmer Penguin Species\n\nAbstract\nIn this blog post, we will use the palmer penguin data set, read in above, to train a logistic regression model to correctly identify the penguin species based off the penguin’s qualitative and qualitative features. We first prep and filter the data, and test different combinations of the penguins features to see which pairing gives us the most accurate prediction of the species. This was determined by looking at the cross validation score for each combination. We then hope to achieve 100% accuracy at predicting the penguin species given this best combination of features.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAbove we are importing data from the specififed URL and then reading in that data to the variable “train” We then can use the command “.head()”, to print ot the data table in a digestable and readable way.\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis code snippet is part of a data preparation process. First we have to import LabelEncoder class from the sklearn.preprocessing module, to help use encode labels into numerical values. We then call LabelEncoderand fit it to the species columb of the tranining data. This turned our categorial labels into number digestable to the machine learning algorithm. Next, a function named prepare_data is defined to handle the preprocessing tasks. Within this function, we remove irrelevant columns such as “studyName,” “Sample Number,” and others specified in the drop() method, we filter out rows with missing values in the “Sex” column, and if there are any missing values the rows are dropped as well.pd.get_dummies()switches the categorical columns into binary encoded columns and the now prepared data set is returned.\nWe then split the data into ‘x_train’ and ‘y_train’ and call the new prepare_data function described above on our data.\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nAfter importing our necessary platforms, we then set up the cross valadation score, establishing the classifier as a random object with no parameters. CV is the amount of time we cross-validate and then return our mean scores. The provided block of code defines a function named evaluate_features that assesses the predictive performance of a set of features using cross-validation with a random forest classifier. In the ’scores array, the accuracy scores are computed adn stores before returning the mean of these scores.\n\n# Combination copied from assignemnt\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for  quality interactions                                                                                                                                                                        \nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nbest_columns = []\nbest_score = 0\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n\n    # select the combination that has the best score. \n    if cv_scores_LR &gt; best_score:\n            best_score = cv_scores_LR\n            best_columns = cols\n     \nprint(\"Best columns:\", best_columns)\nprint(\"Best score:\", best_score)\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nBest columns: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\nBest score: 0.9844645550527904\n\n\nIn the code snippet above, we define our qualitative columns and our quantative columns, and then with a nested for loop over our data we combine our columns. Finally we print out the combinations so we can get a sense of visualizing our data.\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguins.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove, the code prepares the data and then preforming a train_test spilt. We then choose which columns we want to be our qualitative data versus quantitative data and defining our classifiers.\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[best_columns], y_train)\nLR.score(X_train[best_columns], y_train)\nLR.coef_\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\narray([[-0.88566361,  1.76871749, -0.55903277,  0.51289582],\n       [ 0.40297701,  0.52346615,  1.25916366, -1.22691883],\n       [ 0.4826866 , -2.29218364, -0.7001309 ,  0.71402302]])\n\n\nIn the code snippet above, we are now actually preforming the logicstic regression. We fit the logistic regression to our training data, highlighting which predicator column we want, then we calculate the scores.\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\npreds = LR.predict(X_test[best_columns])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with some accuracy!\n\n\nDecision Regions\nThe code below is taken form Professor Phil’s notes. My predictor_cols only has two columns so I had to modify the code to reflect this\nRetraining the data on all the columns, instead of just the predictor columns\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = cols[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nprint(best_columns)\nplot_regions(LR, X_train[best_columns], y_train)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion\nThroughout this blog post, we used the Palmer Penguin data set to implement logistic regression with combinations. We determined that the combination that predicts the penguin species with 100% accuracy is both “Culmen Length” and “Culmen Depth”. I ran into challenges with over fitting the model to my training data, and therefore was still a bit skeptical when the model predicted 100% accuracy with only two penguin features. I wonder if we grew the data set to add more penguins if there would be outliers? Or if this the combination of culmen depth and length has biological certainty to predict penguin species. Overall it was very satisfying and interesting ot create and implement a model that gives us 100% accuracy."
  },
  {
    "objectID": "posts/Blog 4/Blog5.html",
    "href": "posts/Blog 4/Blog5.html",
    "title": "Abstract",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nAbstract\nIn this blog post, I will implement perceptron on linearly separable data, non-linearly separable data, and 5d data, as well as visualizations and comments to describe what each code block is executing. That data I wil be using is taken form the class notes and then modified to fit the criteria I listed above. I will learn and test how the perceptron algorithm works by evaluating its performance on different data forms. Some data points and visualizations are taken from the class notes so credit goes to Professor Phil!\n(Here is the link to my perceptron file, https://vscode.dev/github/lenoxherman/lenoxherman.github.io/blob/main/posts/Blog%204/perceptron.py )\n\n\nImplementing Perceptron\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(12989)\n\ndef perceptron_data(n_points = 100, noise = 0.23, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 100, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere is example code taken from a warmup, that shows linear separable data. I will then run my implemented perceptron on this data. Look into my perceptron python file to find where I implemented my own grad and step functions.\n\nfrom perceptron import Perceptron,perceptron_data, PerceptronOptimizer\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nTo test out my implementation of perceptron, I ran a minimal training loop using the data from above.\n\ny\np.predict(X)\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe graph above shows the steps of my implemented algorithm as the loss approaches 0. After a little over 1400, the loss is at 0!\n\ndef accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    predictions = 2*predictions - 1\n    \n    preds = (predictions == y).float()\n    accuracy = torch.mean(preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\naccuracy(X, y)\n\nAccuracy: 1.0\n\n\nThe function above test the accuracy of our model and as seen above we have 100% accuracy!\n\n\nVisualizations\nWe use the data from minimal training loop to show the line that linear seperates the data.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\nThis code was taken from the notes, and draws a line between the two sets of points.\n\nImplementing Perceptron on Data that is not Linearly Separable\nI made a new plot below that has overlapping points, so the data is no longer linear separable\n\nX, y = perceptron_data(n_points = 50, noise = 0.7)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere the line seperating the two data sets, isn’t very accurate and doesn’t do a good job splitting up the data\n\nX, y = perceptron_data(n_points = 50, noise = 0.7)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe can from the visualizations above, that the data is not linearly separable, and its impossible to draw a line that splits the two datasets\n\nfrom perceptron import Perceptron,perceptron_data, PerceptronOptimizer\nimport torch\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\n\nloss = 1.0\nmax_iterations = 1000\n\nn = X.size()[0]\n\nwhile loss &gt; 0  and max_iterations &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[i,:].squeeze()\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iterations -= 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAbove is a graph, once again mapping the loss as it approaches 0. This time after 1000 iterations, the loss ossilates a lot, and never converges at 0.\n\n\nImplementing Perceptron on Data that is not 2D\nThis code will run perceptron on 5D data.\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,5))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.35)\n\n\nX, y = perceptron_data()\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\nmax_iterations = 1000\n\nloss = 1.0\n\nn = X.size()[0]\n\nwhile loss &gt; 0  and max_iterations &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[i,:].squeeze()\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iterations -= 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAfter 1000 iterations, the loss is almost at 0!\n\naccuracy(X, y)\n\nAccuracy: 0.9866666793823242\n\n\nWith 5D data, after 1000 iterations the loss is almost at 0 and our accuracy is at 98.6%, which is almost 1000, very similar to when we implemented perceptron on not linearly separable data.\n\n\n\nConclusion\nIn this blog, I coded my own version of the perceptron algorithm, which is provided in the attached Python file. Testing this implementation on linearly separable data revealed that the loss approached 0, indicating successful classification. However, when applying the algorithm to non-linearly separable data, it became evident that convergence might never occur, potentially causing the algorithm to run indefinitely. To address this issue, I introduced a maximum iteration limit to prevent infinite execution. Despite this limitation, the accuracy achieved was nearly 100%. Nevertheless, it’s important to note that the perceptron algorithm may not achieve perfect separation between the two datasets. Furthermore, I extended the application of the perceptron to 5D data, achieving a loss of 0 once again. Overall, my implementation of the perceptron algorithm proved successful across all three types of datasets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Linear Regression Blog Post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguin Species\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Blog Post\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nLenox Herman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Working Blogs/Warmup3:28.html",
    "href": "Working Blogs/Warmup3:28.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "def mystery_fun(a, epsilon, alpha, max_iterations = 100):\n    # Initial guess\n    x = a \n    xPrime = 0\n    j = 0\n\n    # Set iteration counter and maximum number of steps\n    \n    while abs(xPrime - x) &gt; epsilon:\n        # Calculate the value of f(x) using the given formula\n        \n        # If the absolute difference between f(x) and 0 is less than tolerance, break\n        if j &gt; max_iterations:\n            break\n        loss = x - (a / x)\n        xPrime= x\n        x = x - alpha * loss\n        j += 1\n    \n    return x\n\n# Testing the function\n# Test 1: Setting of a for which the function returns a real number very close to the exact value of sqrt(a)\nresult1 = mystery_fun(a=9, epsilon=1e-8, alpha=0.2)\nprint(\"Result 1:\", result1)\n\nresult2 = mystery_fun(a=9, epsilon=1e-8, alpha=2)\nprint(\"Result 2:\", result2)\n\nresult3 = mystery_fun(a=9, epsilon=1e-8, alpha=.001)\nprint(\"Result 3:\", result3)\n\n\nResult 1: 3.000000013459385\nResult 2: -76.62213126914914\nResult 3: 8.235542835355941"
  },
  {
    "objectID": "Working Blogs/linear regression post /linearRegression.html",
    "href": "Working Blogs/linear regression post /linearRegression.html",
    "title": "Linear Regression Blog Post",
    "section": "",
    "text": "Linear Regression Blog Post\n\nAbstract\nIn this blog post, I will try three different experimentation of logistic regression. In class we often use math plot lib’s built-in Linear Regression; however, in this blog post I have implemented my own linear regression model (logistic.py). I first implement regular (vanilla) gradient descent, I then add momentum to the model and explore the difference between the two. Lastly, I show what happens to the testing accuracy when a model is over fitted to the training data.\nhttps://vscode.dev/github/lenoxherman/lenoxherman.github.io/blob/main/Working%20Blogs/linear%20regression%20post%20/logistic.py#L6\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nExperimentations\n\nimport torch\n\n\ndef classification_data(n_points = 500, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.2)\n\nThis is random data taken from the blog post description. The number of points was changed from 300 to 500 for added complexity. The noise is the extent that the two classes of data overlap. I decided to create a dataset where the noise equals .2.\n\nfrom matplotlib import pyplot as plt\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis plot function as taken from the perceptron blog post. It shows the data set, which is currently linearly separable. Let change that! Lets increase our noise to .4.\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe now wan to to train our model and experiment it on this new data set. We want to first try to implement Vanilla Gradient Descent. This model does not have momentum within the step function and has a relatively small learning rate, so it is a simplified version to test if our original implementation is effective\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nalpha = 0.01\n\nfor _ in range(9000):\n    \n    opt.step(X, y, alpha, beta = 0)\n\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n\nplt.plot(loss_vec, color = \"blue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"blue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Loss function over time for Vanilla Gradient DEscent\")\n\nText(0.5, 1.0, 'Loss function over time for Vanilla Gradient DEscent')\n\n\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 5))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_classification_data(X, y, ax)\ndraw_line(LR.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nAfter 9000 iterations, the graphs above shows our loss function. The code attempting to separate the data points was taken from the notes. It shows a line that is not 100% accurate but does a decent job attempting to separate the data.\nNow we are going to implement momentum to our gradient descent and see how it effect our loss function\n\nloss_vec_two = []\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(9000):\n    opt.step(X, y, alpha, beta = 0.9)\n    \n    loss_1= LR.loss(X, y) \n    loss_vec_two.append(loss_1)\n\n\n\nplt.plot(loss_vec, color=\"purple\", label=\"Vannila Gradient Descent\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color=\"purple\")\n\nplt.plot(loss_vec_two, color=\"pink\", label=\"Gradient Descent with Momentum\")\nplt.scatter(torch.arange(len(loss_vec_two)), loss_vec_two, color=\"pink\")\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Evolution of Loss Function - Comparison\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the Gradient Descent with momentum reaches the same loss as the Vanilla Gradient Descent 6000 iterations earlier.\nif we generate data were the p_dims is greater than our number of points than twe run the risk of overfitting to our training data. For my data I have each of my data sets have 100 points and my p_dims is 200. I split up the data into X_train and X_test to ensure I am training on a different data set than I am testing on.\n\nX_train,y_train = classification_data(n_points= 50, noise = 0.4, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.4, p_dims = 100)\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere is our data! We have 50 data points and 100 features.\nWe then implement linear regression on our training data.\n\nLR_2 = LogisticRegression() \nopt = GradientDescentOptimizer(LR_2)\n\nloss_vec_three = []\n\nfor _ in range(100):\n\n    opt.step(X_train, y_train, alpha = 0.4, beta = 0.9)\n    \n    loss_three = LR_2.loss(X_train, y_train).item()\n    loss_vec_three.append(loss_three)\n\n\npreds = LR_2.predict(X_train)\naccurate_preds = (preds == y_train).sum()\ntrain_acc = accurate_preds / len(y_train)\ntrain_acc.item()\n\n1.0\n\n\nWe have 100% accuracy on our training data. Now lets apply that to our testing data.\n\npreds = LR_2.predict(X_test)\naccurate_preds = (preds == y_test).sum()\ntrain_acc = accurate_preds / len(y_test)\ntrain_acc.item()\n\n1.0\n\n\nWe only have 95% accuracy on our testing data, still pretty close. However, since we overfit to our training data, our training data gives 100% accuracy while our testing data only reports 95% accuracy.\n\n\nDiscussion\nOverall, this blog post taught me what background equations and computations happen within the logistic regression model. I learned that introducing momentum to the linear regression model, converges much quicker than a model without momentum. I also learned and showed the danger of overfitting a model to training data, and how it can lead to decreased accuracy on testing data. This is especially true if the dimensions of the data is more than the number of points. Overall, it was cool the debunk the complexity and mystery surrounding logistic regression and see the different elements it requires to produce a desired outcome."
  },
  {
    "objectID": "posts/LinearRegression/linearRegression.html",
    "href": "posts/LinearRegression/linearRegression.html",
    "title": "Linear Regression Blog Post",
    "section": "",
    "text": "Linear Regression Blog Post\n\nAbstract\nIn this blog post, I will try three different experimentation of logistic regression. In class we often use math plot lib’s built-in Linear Regression; however, in this blog post I have implemented my own linear regression model (logistic.py). I first implement regular (vanilla) gradient descent, I then add momentum to the model and explore the difference between the two. Lastly, I show what happens to the testing accuracy when a model is over fitted to the training data.\nhttps://vscode.dev/github/lenoxherman/lenoxherman.github.io/blob/main/posts/LinearRegression/logistic.py#L3\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n\n\nExperimentations\n\nimport torch\n\n\ndef classification_data(n_points = 500, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.2)\n\nThis is random data taken from the blog post description. The number of points was changed from 300 to 500 for added complexity. The noise is the extent that the two classes of data overlap. I decided to create a dataset where the noise equals .2.\n\nfrom matplotlib import pyplot as plt\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis plot function as taken from the perceptron blog post. It shows the data set, which is currently linearly separable. Let change that! Lets increase our noise to .4.\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe now wan to to train our model and experiment it on this new data set. We want to first try to implement Vanilla Gradient Descent. This model does not have momentum within the step function and has a relatively small learning rate, so it is a simplified version to test if our original implementation is effective\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\nalpha = 0.01\n\nfor _ in range(9000):\n    \n    opt.step(X, y, alpha, beta = 0)\n\n    loss = LR.loss(X, y) \n    loss_vec.append(loss)\n\n\nplt.plot(loss_vec, color = \"blue\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"blue\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\nplt.title(\"Loss function over time for Vanilla Gradient DEscent\")\n\nText(0.5, 1.0, 'Loss function over time for Vanilla Gradient DEscent')\n\n\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 5))\nax.set(xlim = (-2, 3), ylim = (-2, 3))\nplot_classification_data(X, y, ax)\ndraw_line(LR.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nAfter 9000 iterations, the graphs above shows our loss function. The code attempting to separate the data points was taken from the notes. It shows a line that is not 100% accurate but does a decent job attempting to separate the data.\nNow we are going to implement momentum to our gradient descent and see how it effect our loss function\n\nloss_vec_two = []\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(9000):\n    opt.step(X, y, alpha, beta = 0.9)\n    \n    loss_1= LR.loss(X, y) \n    loss_vec_two.append(loss_1)\n\n\n\nplt.plot(loss_vec, color=\"purple\", label=\"Vannila Gradient Descent\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color=\"purple\")\n\nplt.plot(loss_vec_two, color=\"pink\", label=\"Gradient Descent with Momentum\")\nplt.scatter(torch.arange(len(loss_vec_two)), loss_vec_two, color=\"pink\")\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Evolution of Loss Function - Comparison\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, the Gradient Descent with momentum reaches the same loss as the Vanilla Gradient Descent 6000 iterations earlier.\nif we generate data were the p_dims is greater than our number of points than twe run the risk of overfitting to our training data. For my data I have each of my data sets have 100 points and my p_dims is 200. I split up the data into X_train and X_test to ensure I am training on a different data set than I am testing on.\n\nX_train,y_train = classification_data(n_points= 50, noise = 0.4, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.4, p_dims = 100)\n\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere is our data! We have 50 data points and 100 features.\nWe then implement linear regression on our training data.\n\nLR_2 = LogisticRegression() \nopt = GradientDescentOptimizer(LR_2)\n\nloss_vec_three = []\n\nfor _ in range(100):\n\n    opt.step(X_train, y_train, alpha = 0.4, beta = 0.9)\n    \n    loss_three = LR_2.loss(X_train, y_train).item()\n    loss_vec_three.append(loss_three)\n\n\npreds = LR_2.predict(X_train)\naccurate_preds = (preds == y_train).sum()\ntrain_acc = accurate_preds / len(y_train)\ntrain_acc.item()\n\n1.0\n\n\nWe have 100% accuracy on our training data. Now lets apply that to our testing data.\n\npreds = LR_2.predict(X_test)\naccurate_preds = (preds == y_test).sum()\ntrain_acc = accurate_preds / len(y_test)\ntrain_acc.item()\n\n0.9800000190734863\n\n\nWe only have 95% accuracy on our testing data, still pretty close. However, since we overfit to our training data, our training data gives 100% accuracy while our testing data only reports 95% accuracy.\n\n\nDiscussion\nOverall, this blog post taught me what background equations and computations happen within the logistic regression model. I learned that introducing momentum to the linear regression model, converges much quicker than a model without momentum. I also learned and showed the danger of overfitting a model to training data, and how it can lead to decreased accuracy on testing data. This is especially true if the dimensions of the data is more than the number of points. Overall, it was cool the debunk the complexity and mystery surrounding logistic regression and see the different elements it requires to produce a desired outcome."
  },
  {
    "objectID": "posts/Final Project Blog Post/FinalProject BlogPost.html",
    "href": "posts/Final Project Blog Post/FinalProject BlogPost.html",
    "title": "Final Project Blog Post",
    "section": "",
    "text": "FINAL PROJECT BLOG POST\n\n\nAbstract\nNavigating tipping etiquette at restaurants or shops can be challenging due to inconsistencies across different locations and cultures. In our project, we aimed to analyze the tipping behavior following meals at restaurants. Specifically, we sought to investigate whether various factors, such as gender or party size, influenced tip amounts. To address this issue, we employed predictive models, those being linear regression, decision tree classifier, and random forest classifier, to estimate the expected tip for each individual based on the given variables. We ran these models simply on the data and then with combinations of different columns/attributes. The overall results on our testing data was a score accuracy of about 40% and with signs of overfitting.\nhttps://github.com/lenoxherman/Tippers/blob/main/ProjectFinal.ipynb https://github.com/lenoxherman/Tippers/blob/main/GDO.py\n\n\nIntroduction\nIn the social dance of dining out, tipping is impossible to navigate and often adds stress and confusion to your dining experience. The custom of tipping, ingrained in many cultures, serves as a form of appreciation for service rendered, yet navigating the nuances of tipping etiquette remains inconsistent across locations and cultures.\nScholars have attempted to uncover what factors influence tipping behavior, looking into an array of variables from physical attractiveness and service quality to demographic factors like age, race, and gender. Lynn (2000) set up their study at a Mexican restaurant in Houston, Texas, scrutinizing the impact of physical attractiveness, service quality, self-monitoring, and gender on tipping behavior. Age and ethnicity were also collected but weren’t used in synthesizing the data. For self-monitoring, the servers were asked to fill out a survey assessing their strengths and weaknesses, including categories such as attentiveness and friendliness. Photographs of the participants were taken, and then a panel of 10 judges, 5 male and 5 female, assessed their physical attractiveness. Each server’s tips were then recorded, making sure that participants had equal amounts of evening shifts and day shifts. The findings of the study showed that tips given during daytime shifts, aka during lunch, did not hold any significant findings. Attractive servers were given higher percent tips than unattractive servers. However this only applied to female servers, not male servers. There was no effect of attractiveness on percent tips for male servers. Also, servers who had more confidence in their abilities and rated themselves higher on the survey also had higher percentage tips.\nJewell (2008) contributed to this body of knowledge by researching the demographic factors in tipping behavior, despite the potential introduction of biases. In this study 97 individuals were evaluated as customers at a restaurant and the waitresses were actors. They looked at interactions between the patron and the servers, evaluating factors like age, race, mealtime, gender, alcohol consumption, and interpersonal touch connection. Overall the study found that surprisingly younger populations left higher tips than the middle age or the elderly. The biggest predictor of tipping behavior was race, the tip average for a black waiter was 7% lower than that of a white waiter. However, white diners also tip more than black diners as well.\nLastly, diners who drank alcohol actually tipped less than those who did order an alcoholic beverage. Adding to this research, Cho (2014) dissected the multifaceted dynamics of tipping behavior, examining variables such as bill size, party size, and patron gender. Business students at an urban university were asked to keep a tipping log, keeping track of where they ate, when, and what tip they left behind. Contrary to the Jewell study, they found a positive relationship between tip amounts and whether or not the party ordered alcohol. They established a relationship between tip size mealtime and bill size, with dinner and bigger bill size tipping more. Lastly, they established a connection between gender and tipping amounts, finding that female costumes often tipped higher.\nAs you can tell, the empirical evidence is contradictory and confusing. Factors that are proved significant in one study, their inverse is proved true in the next. The studies all emphasize the importance of setting and context when tipping and the high variability in tipping behavior can be explained by the social norms and context dependent decisions being made.\nGiven the fragmented nature of empirical evidence surrounding tipping behavior, our project seeks to address this complexity by employing predictive modeling techniques. Focusing on tipping behavior following meals at restaurants, we aim to analyze the influence of factors like gender and party size on tip amounts. Through this approach, we endeavor to provide a more nuanced understanding of tipping etiquette, moving beyond anecdotal observations to uncover underlying patterns and dynamics.\n\n\nValues Statement\nThe potential users for our project we imagined would be restaurants, or in general, business owners as well as service staff such as waiters and waitresses. Others who might be affected by this project include the restaurant or business customers, the community surrounding the restaurant, and researchers to name a few.\nAs mentioned, the idea behind our project was for business owners who rely on tips as well as waiters/waitresses to be able to use the information and results to their advantage to get more tips and provide customers a better environment at restaurants. The results could help them make strategic decisions on pricing and staffing. Additionally, this project could be beneficial to consumer researchers and those studying predictive modeling because they can take the findings or get ideas from this project to model more research. Others who might be affected by our project include the customers who attend these restaurants as they might be able to benefit from the improved restaurant quality, or they can also be negatively impacted by being exploited by businesses. The communities where the restaurants are located could also be socio-economically impacted because with improved dining services comes increase in service and menu prices, thus, affecting their ability to eat at the restaurants near them.\nOur personal reason for choosing this project was because we were curious in finding out what factors influence tipping behaviors in restaurants. We wanted to uncover patterns that would allow restaurant owners to make informed decisions for their restaurant. Furthermore, we wanted to apply the knowledge we gained in class to a real-world data set. We believed exploring this dataset and running predictive models on it would bestow us the opportunity to practice our data analysis skills, data processing, and classification skills.\nI believe the world of restaurants and businesses that rely on tips would be a better place with this technology as long as they do not take advantage of their customers and make them tip unfair amounts.\nThe world in general will also be more equitable as restaurant staff will be making the income they should if it is a restaurant that has great service for guests. However, there might be unpeaceful situations such as if there are gender biases found and restaurants exploit this information. There will also be less joy if restaurants decide to up their prices for improved customer service and the lower socio-economic families that lived around the restaurant can no longer afford to eat there. It is difficult to say we are sure that this implementation will benefit the world because there will always be groups who get the short end of the stick. However, it is also unfair to say it would make the world a worse place because for the people it does benefit, it will improve their lives.\n\n\nMaterials and Methods\nOur data\nThe data was collected from a dataset website known as Kaggle and the collaborator of the dataset is Sakshi Satre (Sat Tips Dataset, 2024). This dataset is used for visualizations in data analysis and it contains information about different factors of customers in a restaurant, such as the total bill amount, tip amount, gender, whether the customer is a smoker or not, the day of the week, time of day (lunch or dinner), and the size of the party. Below are the descriptions of each of these factors as written in the dataset:\ntotal_bill: This attribute represents the total amount of the bill paid by the customer, including the cost of the meal, taxes, and any additional charges.\ntip: This attribute denotes the amount of tip left by the customer. It is typically calculated as a percentage of the total bill and is often discretionary. (This helped form our target value)\nsex: This attribute indicates the gender of the customer. It could be either male or female.\nsmoker: This attribute indicates whether the customer is a smoker or a non-smoker. It’s a categorical variable with two possible values: “Yes” for smokers and “No” for non-smokers.\nday: This attribute represents the day of the week when the meal was consumed. It could be any of the seven days in a week (e.g., Monday, Tuesday, etc.).\ntime: This attribute denotes the time of the day when the meal was consumed. It’s often categorized into two values: “Lunch” for meals consumed during the day and “Dinner” for meals consumed in the evening.\nsize: This attribute indicates the size of the party dining together. It represents the number of people included in the bill.\nThere were a few limitations of the data given that some identifying attributes that impact how much a person tips is based on age and race of the customer. Additionally, the sex of the waiter would be important to note to see if the tip given would be impacted on if the customer was a male and the waitress was a female. Another limitation is the small number of customers/rows in our dataset. We had a little less than 250 rows which is not ideal for training and testing. The type of restaurant, such as if it is a dinner or a sports bar and grille, is also not included. This is important information because that also plays a role on how much is being tipped as it depends on the customers that eat at those restaurants. For example, if a restaurant is higher-end, that will attract people on the wealthier side, thus they might tip more compared to customers from a lower-end restaurant.\nOur Approach\nFrom the features listed above, our target value was created using the tip column. Using the total_bill column, we found the percentage each tip from the customer fell into, for example, we saw if the tip was 15% of the bill or 10%. Then from this, we grouped the percent tips into groups, that is, if the percent fell into a group of 0-10%, 10-15%, 15-20%, 20-25%, or over 25%. Lastly, we encoded the groupings so our target value y consisted of either 0, 1, 2, 3, or 4s. For our predictor features, we used the remaining attributes such as the size, the day (encoded), the time (encoded), the total_bill, the smoker (encoded), and the sex (encoded). To train our data, we used the models logistic regression, decision tree classifier, and random forest classifier. We chose these as we wanted to compare what model would provide a higher accuracy score with our data, which was our way of evaluating our models. These were the classifier models we worked on for our penguin blog post, so we decided to test them with this data as well. Furthermore, taking more inspiration from the penguins blog post, we decided to also try to see if combinations of different features would affect the accuracy score for each model. We were interested in seeing what factors seemed to classify the tips. As mentioned, we evaluated our models based on the accuracy score and had our testing set be 20% of our data.\n\n\nResults\nWe started with a dataset with a little under 250 rows. We split this 80/20 into both our training and test data. After training and running our models our results for every model’s testing data set did not reach 50% accuracy. To make sure our data was not being overfitted, we found the base rate of the data, and it turned out to be about 39% accurate. If our testing results were performing better than this, then we would not have cases of overfitting and our models, given the data, despite being below 50%, would be implemented correctly.\nFor example, with logistic regression, our training data gave us a score of 45.1% accuracy. This is higher than our base rate, 39%, but still not ideal accuracy. After training, we ran logistic regression on our testing data and still achieved a higher accuracy score than the base rate, this time getting a score of 42.8%. Since our data set is small, we can see a little bit of overfitting to the training data, but nothing too significant. We then ran logistic regression, this time implementing the best combinations of our columns, this gave us an accuracy of 44.6% with our training data. When we then applied this to our testing data, we obtained an accuracy score of 34.6%. Here, the combinations improved the score of our training data but decreased the accuracy score of our testing data. This is possibly due to the overfitting of the data.\nWith our second model, the decision tree classifier, we got the best score of about 43.1% accuracy on the training data with it being found at depth 2. This is a lower training accuracy than logistic regression’s score, but it is still above our 39% base rate. On the testing data, the accuracy increased compared to the training data, which resulted in about a 43.2% accuracy rate found at the first depth. This score accuracy was also above the base rate, but still not ideal. Next, we ran the algorithm to find the best combinations of our training data columns and found that the best columns were the total bill, size, and day encoded columns at depth five with an accuracy of about 44.6%. When we ran these best columns through to be fitted and scored on the training data, we got an accuracy of about 77.4%, which resulted in being the best score we got for the entire models. This was higher than running the model without combinations on the training data, which was a positive result. However, the jump from 44.6% to 77.4% lets us know there is overfitting in our data because this significant jump did not occur in the testing combinations data. For example, in the testing data column combinations, we found the best columns were also total bill and sex encoded at depth nine with an accuracy score of about 47.2%. When we fitted and scored the best columns, we got an accuracy score of about 32.7%, which is significantly lower than the training combination result and below the base rate. Thus, there is overfitting in the data due to the smaller size of the dataset.\nLastly, we implemented a Random Forest Classifier to see if that would improve our prediction score. Here, our testing data gave us an accuracy score of 44.9%, which is right around the scores we had seen from our other models. We then implemented the model on the training data with combinations and received an accuracy score of 44.6%. In this model, combinations did not help to improve the accuracy score on training data. We then applied this model to the testing data and saw an accuracy score of 34.7% which once again s\n\n\nConcluding Discussion\nOur project was able to improve the base rate prediction score on all three of our implementations.In terms of our planned deliverables, we were able to produce a well-documented Juniper notebook with code that runs without errors. Our final project has both graphs and a research component. We were most successful in staying on topic and adhering to the timeline we outlined in the project proposal. We were able to complete our weekly assignments and, therefore, produced all the code that we had envisioned, even if the outcome wasn’t quite what we had hoped. Unfortunately, we were not able to complete our own implementation of Logistic regression. We spent most of our time debugging and working on that Python block but could not get the code working on time. If we were to continue the project, our first priority would be to modify the Python package without our logistic regression. Regarding alternate definitions of success, Daniela and I were successful in learning more about implementing these models. Our coding speed increased exponentially from the first week to the last. Originally, we had to think through what we wanted our code to accomplish and then refile past blog posts, notes, and the internet to find out how to start. By the end, we were able to think through what the code needed to do and start implementing it within the same sitting. This process is probably even more noticeable through debugging. At the beginning of the project, even identifying the problem took lots of time and probably several office or peer help hours. However, by the end, we were able to look at an error message or discrepancies in outcomes and identify what was wrong and how to correct it on our own (mostly). As I mentioned in our presentation, our biggest limiting factor was our data. The data we chose was a very small sample, which led to issues within our project, like overfitting and low prediction scores. With a more robust data set, both in terms of rows (our data points gathered) and columns (variables that might have affected tipping behavior), I think we would see a major increase in our prediction scores. Empirically, variables such as the tipper’s age or the server’s race have been found to influence tipping behavior significantly (Lynn et al., 2008), . There is also empirical evidence on how tips are higher on weekends and female servers receive higher percent tips than their male counterparts (Lynn, 2000). These variables are represented in our data, so it would be interesting to see if combining all these factors would increase our prediction score. However, it is risky to invite data about race, age, and even sex because it allows for bias to seep into the data. In the articles we examined, influential factors such as gender and mealtime significantly shaped tipping behavior, both of which were encompassed within our dataset. However, our data lacked robustness, failing to encompass other crucial variables such as the age of the patron (Jewell, 2008). Moreover, prior research hash highlighted the necessity for more contextual data, including the mood and attitude of both the waiter and the patron (Lynn & Simons, 2000). Regrettably, our dataset offered a narrow perspective on tipping behavior, focusing solely on server attributes. If replicated, it would be advantageous to augment the dataset with information regarding the patron and the restaurant, thus providing a more comprehensive understanding of tipping dynamics.\n\n\nGroup Contributions Statements\nDaniela: For the source code, Lenox and I did paired programming where we both coded the project while we were together. We took turns with one person typing and the other walking the typer through what we were coding. There were times we worked apart to do visualizations or debugging and for that, I did the first visualization where the graph shows the correlation between the size and total bill based on the time the customer went to eat at the restaurant. I also worked on part of the formation for the last graph depicting the total bill by party size and gender. Furthermore, my laptop would have issues loading the code at times, so we worked from Lenox’s computer a few times too. For debugging, while at times it was done separately, we often met to go to office hours to get help on them if we could not figure it out. For the blog post, I wrote the abstract which we had thought of together previously, then wrote the value statement, the materials and methods, and finally wrote the results together with Lenox. For the final presentation, we split the slides evenly so each of us did two slides. I worked on the “About the Project” and the “Results of Testing Data” slides.\nLenox: For our project, we split up the workload evenly between the two of us. For the presentation, we split it down the middle each doing two slides. I did the slide explaining our approach to the data and what models we implemented as well as the slide that explained our results and offered improvement if the project was to be replicated. For the coding portion, Daniela and I met 3-5 times a week for 2 hours session to do pair coding. Due to merging issues early on, we took turns with one person physically coding and the other looking for old blog posts and notes to help out. We did some coding on our own; although the majority of it took place together. I did the last three visualizations of the data in the visualization section and I read all the academic journals for this project, writing up the background section at the bottom. Because I was working with the empirical data for this project. I wrote both the introduction section and the conclusion for the project write up. I also made the bibliography file, since I had access to the sources. Daniela and I wrote the results sections together (as well as this section of course).\n\n\nPersonal Statement\nThis project has taught me more than the blog posts combined. Working with data for a long time allowed me to really understand what our data was telling me; therefore, affording more time about what to do with it and decide how I want it to be implemented. Implementing logistic regression from scratch, really helped me to understand what exactly logistic regression does and the different components it needs to function. I also feel like I have a better mathematical grasp of different machine learning models that we implemented in our project, like what a random forest classifier or a decision tree classifier does. I think the most helpful portion of this project was understand how data sets works and what goes into “cleaning a data set”. I really enjoyed manipulating columns and encoding qualitative data to have the data be able to fit into our models. This also made the visualization section of the project, much easier because I was able to fully understand our data.\nI think we did a good job working with the data we had. Although the results were far from what we had hoped, we spent a lot of time on the code. Even if it doesn’t show, and there was a large learning curve when we were working with our data. We were able to work so much faster and accomplish so much more in a sitting by the end of the project. Our project does a good job implementing the different models that we have seen in class, even if they are a little more simplistic. I wish that we had more time to get our own version of logistic regression working on our data set, but we just didn’t have the time. I also would have loved to see a higher accuracy score, but I think that is more at fault of the data rather than our implementation of it.\nI had never really worked with data (or linear algebra or python!) so this class and this project carries limitless applications for me. I now feel much more confident pursuing a career that works with data, and as you are sure, this opens so many doors for me. Also as AI prevalence increases, I think it is so important to have a background in machine learning, and I know get to sound smart when I talk about neuroautoencoders! .\n\n\nReferences\nCho, Sun Bai. “Factors Affecting Restaurant Consumers’ Tipping Behavior.” Journal of the Korean Society for Quality Management, vol. 42, no. 1, 31 Mar. 2014, pp. 15–32, doi:10.7469/jksqm.2014.42.1.015.\nJewell, Cassie N. “Factors Influencing Tipping Behavior in a Restaurant.” Psi Chi Journal of Psychological Research, vol. 13, no. 1, 2018, pp. 38–48, doi:10.24839/1089-4136.jn13.1.38.\nLynn, M. “Restaurant Tipping and Service Quality a Tenuous Relationship.” The Cornell Hotel and Restaurant Administration Quarterly, vol. 42, no. 1, Feb. 2001, pp. 14–20, doi:10.1016/s0010-8804(01)90006-0.\nSanjanabasu. “Tips Dataset.” Kaggle, 14 May 2020, www.kaggle.com/code/sanjanabasu/tips-dataset. Accessed 16 May 2024."
  }
]