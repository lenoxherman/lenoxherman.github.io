[
  {
    "objectID": "Working Blogs/Blog5.html",
    "href": "Working Blogs/Blog5.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nfrom perceptron import Perceptron,perceptron_data, PerceptronOptimizer\nimport torch\n\nX, y = perceptron_data()\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nmax_iterations = 1000\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0 and max_iterations &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[i,:].squeeze()\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iterations -= 1\n\n    \n\nAttributeError: 'PerceptronOptimizer' object has no attribute 'loss'\n\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X, y)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n^^ code copied form notes to draw a line seperating the data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html",
    "href": "posts/Mid course eval/mid-course.html",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Lenox Herman\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I have missed 1 class thus far. It was the Thursday before break and I left for my trip early.\nHow often have you taken notes on the core readings ahead of the class period? I usually don’ take notes on the readings, I look at them and skim them but rarely annotate. The readings that deal with Math, especially math required to complete the warmup, I will mark up and takes some notes on to reference back to.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have done the warmup every time! (One of my goals) however on some of the math ones I have been grateful that I was not selected because I was confused by the notation/ questions and therefore had to skip certain sections that me, the reading, and Google couldn’t answer.\nHow many times have you actually presented the daily warm-up to your team? I have presented the Warmup a twice\nHow many times have you asked your team for help while presenting the daily warm-up? Since we have bene working in the same groups, I have grown comfortable with my group and frequently ask them questions. Usually it is when the warmup involves math, I make them go through the explanation a little slower, or ask them how they cam to their answer. Beyond that as well I ask for their feedback. Even today, I had a loose idea for a project and asked my group how they would rearrange the project to make it better.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Honestly, almost every time. People often have totally different approaches than me, whether its math or a discussion question, someone always answers in a way that I hadn’t thought of\nHow often have you helped a teammate during the daily warm-up presentation? I am usually the one asking the questions, or requiring help in the warm up so not as much as I would like but I hope to improve on that\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? So much. Alex and I and tight now. I go every almost Sunday and Monday and then some Wednesdays to see Steven. I usually attended 3-5 hours of Peer help hours a week for this course.\nHow often have you asked for or received help from your fellow students? I have asked a few people who I have also seen in help hours for help if I see we are working on similar assignments.\nHave you been regularly participating in a study group outside class? No. Almost all the work I do for this course is individual thus far, but I am open to that changing! and excited for our group project.\n\nHow often have you posted questions or answers in Slack? I have communicated with peers via Slack for help, but have not posted questions in the questions chanel.\n\n\n\n\n\nHow many blog posts have you submitted? 2… I have revised one them after receiving feedback so 2.5\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 1\nR: Revisions encouraged: 0\nN: Incomplete: 0 I am resubmitted the blog that received an M, with the suggested edits, so hopefully that changes to an E!\n\nRoughly how many hours per week have you spent on this course outside of class? I work on my blog posts every week, it is just a little challenging, because I do not make a lot of progress without someone looking over my shoulder to edit my code. A lot of this work happens during peer help hours. Warmups, Blogs, and Help hours included I spend roughly 7 hours a week on this course.\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest? I chose to focus on social responsibility and I really enjoy the week that we discussed this in class. The readings were fascinating and helped me to participate more in the discussions. However, I also wanted to challenge myself and improve my Python skills, given this is my first time learning and using it. There has definitely been a big learning curve and I am still gaining more knowledge each time I work with it, but I am definitely a lot more comfortable with the syntax and how it is set up. I have watched some Youtube videos about how it differs than Java, and done some 145 level coding (games that implement for loop, recursion, etc) to get comfortable with its syntax.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nThis is the area of this course that I struggle most with and am also the most behind in. My goal for blog posts in this class were to complete more than half and have more than half of the blogs I did complete be Es. So far I have not accomplished this goal seeing as I have only turned in 2 blog posts. The good news is both of Blog posts are Es so I am at least achieving the second half of my goal, though one of them did require a revision. I am currently working on blog 5, but I have ben working on it for about 2 weeks. I was averaging a blog biweekly, but as the material get more complicated, it is taking me longer to complete the blogs. It is a little frustrating because I am dedicating the same amount of time as before, but I am confident I can catch up on my original goals. I think my original goal is still achievable, I just will need to dedicate more time to blog instead of other aspects of the course.\n\n\n\nThe good news is I am on track for most of my participation goals. I have completed the warmups and been ready to present, even though I wasn’t asked to most times. I also have come to most classes. I hope to maintain this behavior, especially as the weather gets nicer. I think I can improve by participating more beyond our small groups. I have no problem speaking up and sharing my opinion in our small group, but struggle when the audience is the whole class. There have been times that I wanted to share or answer a question, but I wasn’t confident enough in my response. In the second half of this course, and especially as we move into our projects, I want to try to push past that and speak up more even if I am not 100% positive in my answer.\n\n\n\nAs for the project I am very excited about getting rolling. Hearing everyone’s pitches was fascinating and there were quite a few that I found more interesting than my own! I was so inspired by a few that I decided to rewrite my pitch. As for my goals, they mostly remain the same. I am looking to contribute as much as possible, but recognize that my coding style is probably sower than most. I will focus on contributing to the project in other ways, organization, making sure everyone know what we are working on and when. Putting some of my product management skills to use, fascinating meetings and making sure we are on schedule and theme.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI would love to hear from you, what I can improve on. My reflection from the course is very one sided since I can only speak on what I am feeling and struggling with, but as someone who has access to everyone else’s goals, achievements, and workload I would love to hear what things I should be doing more of and how I engage more in the course\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nSo far my goals have been achievable, while still challenging me. I have not met my goal for blogs post, but I think it is a good challenge to keep that goal and try to complete some more blog posts especially as the project ramps up.\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of B+ / A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI have grown a lot in this course with math, Python, but also how conceptually I think about machine learning algorithms and how I think about the bias in the algorithms I interact with daily. However, I am behind on my goals, specifically my blog post goal, so I thats why I am not giving myself an A.\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nGoing forward, I hope to at least maintain the same effort and participation form the first 5 weeks if not more. I think I can do a better job speaking up in larger groups, and will try to be intentional about this in my project group. I want to complete blog 5 and at least one more before the project takes up too much of my time. Sometimes I get dejected by all the things I don’t understand in this class and I enter a state of resignation where I accept defeat. I want to try to push back against that and seek out answers whenever I feel dumb or like I do not know how to approach a problem."
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html#the-data",
    "href": "posts/Mid course eval/mid-course.html#the-data",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "In this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) I have missed 1 class thus far. It was the Thursday before break and I left for my trip early.\nHow often have you taken notes on the core readings ahead of the class period? I usually don’ take notes on the readings, I look at them and skim them but rarely annotate. The readings that deal with Math, especially math required to complete the warmup, I will mark up and takes some notes on to reference back to.\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? I have done the warmup every time! (One of my goals) however on some of the math ones I have been grateful that I was not selected because I was confused by the notation/ questions and therefore had to skip certain sections that me, the reading, and Google couldn’t answer.\nHow many times have you actually presented the daily warm-up to your team? I have presented the Warmup a twice\nHow many times have you asked your team for help while presenting the daily warm-up? Since we have bene working in the same groups, I have grown comfortable with my group and frequently ask them questions. Usually it is when the warmup involves math, I make them go through the explanation a little slower, or ask them how they cam to their answer. Beyond that as well I ask for their feedback. Even today, I had a loose idea for a project and asked my group how they would rearrange the project to make it better.\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? Honestly, almost every time. People often have totally different approaches than me, whether its math or a discussion question, someone always answers in a way that I hadn’t thought of\nHow often have you helped a teammate during the daily warm-up presentation? I am usually the one asking the questions, or requiring help in the warm up so not as much as I would like but I hope to improve on that\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help? So much. Alex and I and tight now. I go every almost Sunday and Monday and then some Wednesdays to see Steven. I usually attended 3-5 hours of Peer help hours a week for this course.\nHow often have you asked for or received help from your fellow students? I have asked a few people who I have also seen in help hours for help if I see we are working on similar assignments.\nHave you been regularly participating in a study group outside class? No. Almost all the work I do for this course is individual thus far, but I am open to that changing! and excited for our group project.\n\nHow often have you posted questions or answers in Slack? I have communicated with peers via Slack for help, but have not posted questions in the questions chanel.\n\n\n\n\n\nHow many blog posts have you submitted? 2… I have revised one them after receiving feedback so 2.5\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 1\nM: Revisions useful: 1\nR: Revisions encouraged: 0\nN: Incomplete: 0 I am resubmitted the blog that received an M, with the suggested edits, so hopefully that changes to an E!\n\nRoughly how many hours per week have you spent on this course outside of class? I work on my blog posts every week, it is just a little challenging, because I do not make a lot of progress without someone looking over my shoulder to edit my code. A lot of this work happens during peer help hours. Warmups, Blogs, and Help hours included I spend roughly 7 hours a week on this course."
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html#what-youve-learned",
    "href": "posts/Mid course eval/mid-course.html#what-youve-learned",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest? I chose to focus on social responsibility and I really enjoy the week that we discussed this in class. The readings were fascinating and helped me to participate more in the discussions. However, I also wanted to challenge myself and improve my Python skills, given this is my first time learning and using it. There has definitely been a big learning curve and I am still gaining more knowledge each time I work with it, but I am definitely a lot more comfortable with the syntax and how it is set up. I have watched some Youtube videos about how it differs than Java, and done some 145 level coding (games that implement for loop, recursion, etc) to get comfortable with its syntax."
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html#reflecting-on-goals",
    "href": "posts/Mid course eval/mid-course.html#reflecting-on-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nThis is the area of this course that I struggle most with and am also the most behind in. My goal for blog posts in this class were to complete more than half and have more than half of the blogs I did complete be Es. So far I have not accomplished this goal seeing as I have only turned in 2 blog posts. The good news is both of Blog posts are Es so I am at least achieving the second half of my goal, though one of them did require a revision. I am currently working on blog 5, but I have ben working on it for about 2 weeks. I was averaging a blog biweekly, but as the material get more complicated, it is taking me longer to complete the blogs. It is a little frustrating because I am dedicating the same amount of time as before, but I am confident I can catch up on my original goals. I think my original goal is still achievable, I just will need to dedicate more time to blog instead of other aspects of the course.\n\n\n\nThe good news is I am on track for most of my participation goals. I have completed the warmups and been ready to present, even though I wasn’t asked to most times. I also have come to most classes. I hope to maintain this behavior, especially as the weather gets nicer. I think I can improve by participating more beyond our small groups. I have no problem speaking up and sharing my opinion in our small group, but struggle when the audience is the whole class. There have been times that I wanted to share or answer a question, but I wasn’t confident enough in my response. In the second half of this course, and especially as we move into our projects, I want to try to push past that and speak up more even if I am not 100% positive in my answer.\n\n\n\nAs for the project I am very excited about getting rolling. Hearing everyone’s pitches was fascinating and there were quite a few that I found more interesting than my own! I was so inspired by a few that I decided to rewrite my pitch. As for my goals, they mostly remain the same. I am looking to contribute as much as possible, but recognize that my coding style is probably sower than most. I will focus on contributing to the project in other ways, organization, making sure everyone know what we are working on and when. Putting some of my product management skills to use, fascinating meetings and making sure we are on schedule and theme.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nI would love to hear from you, what I can improve on. My reflection from the course is very one sided since I can only speak on what I am feeling and struggling with, but as someone who has access to everyone else’s goals, achievements, and workload I would love to hear what things I should be doing more of and how I engage more in the course\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nSo far my goals have been achievable, while still challenging me. I have not met my goal for blogs post, but I think it is a good challenge to keep that goal and try to complete some more blog posts especially as the project ramps up."
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html#grade-and-goals",
    "href": "posts/Mid course eval/mid-course.html#grade-and-goals",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of B+ / A-\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI have grown a lot in this course with math, Python, but also how conceptually I think about machine learning algorithms and how I think about the bias in the algorithms I interact with daily. However, I am behind on my goals, specifically my blog post goal, so I thats why I am not giving myself an A."
  },
  {
    "objectID": "posts/Mid course eval/mid-course.html#optional-how-to-improve",
    "href": "posts/Mid course eval/mid-course.html#optional-how-to-improve",
    "title": "CSCI 0451: Mid-Course Reflection",
    "section": "",
    "text": "You may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nGoing forward, I hope to at least maintain the same effort and participation form the first 5 weeks if not more. I think I can do a better job speaking up in larger groups, and will try to be intentional about this in my project group. I want to complete blog 5 and at least one more before the project takes up too much of my time. Sometimes I get dejected by all the things I don’t understand in this class and I enter a state of resignation where I accept defeat. I want to try to push back against that and seek out answers whenever I feel dumb or like I do not know how to approach a problem."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Blog3/WomeninDataScience.html",
    "href": "posts/Blog3/WomeninDataScience.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "WOMEN IN DATA SCIENCE BLOG POST\nDespite significant advancements in feminist movements, women remain conspicuously underrepresented in fields such as computing, mathematics, and engineering, which not only affects those within the industry but also the industry itself. Diversity fosters informed decision-making and sparks creativity. However, the current representation of women in STEM careers mirrors that of the 1960s, despite increased opportunities and educational access. This disparity, particularly in engineering, stems from historical shifts in the culture surrounding computing and subsequent academic requirements that disadvantage women and marginalized groups. To address this, efforts spotlighting the achievements of women in STEM are crucial. The Women in Data Science Conference that I attended, did just this. Highlighting women working with data in a variety of fields, such as political science, geography, engineering, and computer science the conference made space to discuss the different avenues women have taken with working with data. Additionally, the conference featured an alumni panel to give insight to Middlebury female alums now working in STEM fields. I really enjoyed this section as there is always a gap between education in college and success in the career world, and I enjoyed talking to alums, both recent and no0t so recent about how they used data science to make that jump.\nEven though we got the opportunity to speak with women in data related careers, these industries overall lack female representation. When industries lack proper diversity, perspectives become skewed, hindering unbiased innovation “The United States simply can’t afford to ignore the perspectives of half the population in future engineering and technical designs” (3. Corbett and Hill, 2015). Without diversity and representation in STEM industries, “everyone misses out on the novel solutions that diverse participation brings”(8, Corbett and Hill, 2015). Sometimes for non-diverse groups, the issue isn’t generating the “wrong” or unoptimal solution, its misidentifying the problem altogether.\nRight now, 26% of the population in STEM careers are women. This is an identical representation to STEM as the 1960s, despite the increased opportunity and education of women in STEM. Engineering is the most sex-segregated profession; however, compared to other profession such as lawyers or doctors, the gap between male engineers and female is exponentially wider. In the 1960s, engineering was a new field in need of workers, so it attracted men and women alike. However, when the personal computer was developed in the 80s, it created a hobbyist culture around computers, creating a gamer subculture that became dominated by men. Given the increasingly prevalence of computers both in the home and at work, colleges added increasingly stringent requirements for the computer science major. These additional requirements disproportionately disadvantaged women and people of other marginalized groups who were entering college with less math and programming experience.\nBreaking down barriers and challenging the underrepresentation of women in these fields, can be done by events that spotlighting the achievements of women in STEM, like this panel. Its important to combat workplace hostility towards women, as they are leaving STEM based jobs at a much higher rate than men. By providing visible role models, challenging stereotypes, and building support networks, these events inspire and empower women to pursue careers in STEM. They also advocate for change by raising awareness of systemic issues such as gender bias in hiring and promotion processes. By inspiring future generations of girls and students, these events help to create a more inclusive and diverse STEM community, ultimately contributing to greater gender equality in these fields.\nThis need for diversity within disciplines, extends beyond the STEM communities. Just as the STEM industry grapples with the challenge of inclusively, Amy Yuen’s examination of the Security Council sheds light on the complexities of achieving equal representation in influential decision-making bodies, like the United Nations. These insights show the importance of addressing systemic barriers and promoting diversity to create more inclusive environments conducive to progress and equitable outcomes. Amy Yuen is political science professor, who shared with us how she utilized data and data modeling to determine if the UN security council was “democratic” and offered equal representation. She first shared some necessary background information about the council, describing how there are 5 core countries with veto power who do not move off the council, but 10 other seats in which countries campaign for based on region. Despite the obvious lack of democratic voting within the council, Yuen’s focus was on evaluating the issue of equal representation. Each country holds a rotating elected seat for a month, and Yuen measured success in terms of their output during this tenure. Interestingly, she discovered that factors such as a nation’s wealth or political policies did not significantly impact their monthly output. Instead, she found that sponsorship played a pivotal role in determining the level of output on the council. Despite some countries like Japan or Brazil frequently serving on the council, Yuen noted close to equal representation among rotating elected members.\nSimilarly, Sarah M Brown, our keynote speaker, took a broader approach in investigating equal representation. She came all the way up from the University of Rhode Island to talk to us about the ethics of machine learning and how we can create systems that accurately represent society’s composition. As we are using machine learning more and more, specifically developing machine learning for sociotechnical systems, we need to direct our focus to making machine learning more fair and evaluate the context in which we create these systems.\nHer presentation started off by defining data science as equal parts computer science, stats, and domain expertise. Contrary to common practice, she emphasized the criticality of integrating domain expertise from the outset, rather than merely consulting domain experts later in the process. Contextualizing data, she argued, is key to unlocking solutions to STEM-related challenges. Expounding on this, she presented three fundamental “keys” that have shaped her work. However, she argued that it is important to contextualize this data, and pull upon non computer science or math backgrounds to unlock the keys to solving STEM related issues. The rest of the presentation was an exploration of three “keys” that have helped to unlock her work.\nHer first key was introduced back in 2004 in her high school social studies course. Her teacher described the need to use context to decipher primary sources. Drawing parallels to data as primary sources, Brown stressed the necessity of contextual understanding in deciphering, cleaning, and implementing data effectively. Her second key highlighted the social nuances and disciplinary norms inherent in different fields. She emphasized the need to situate data and models within the framework of the relevant discipline to ensure accurate interpretation and application. As a psych major, I particularly enjoyed this part of the talk. My interest is where the intersect of these two disciplines lie. My “domain skills” are being able to connect people and social leadership with data models and computer science, which can be isolating at times. The last “key” she mentioned was to meet people where they are. She recognized the power of precedent in shaping attitudes towards innovation and change. Using examples, she illustrated the challenges in prioritizing fairness over accuracy, urging for creative solutions rather than mere mathematical refinement.\nShe briefly introduced the concept of “algorithmic reparations,” which is something I had never heard before. Brown provocatively challenged the prevailing notion that accuracy must always take precedence over fairness, highlighting the need for a shift in perspective within the data science community. From a psychological standpoint, she emphasized the imperative of creative thinking in addressing these ethical dilemmas, signaling a departure from traditional approaches grounded solely in mathematical optimization. She ended the lecture on a positive note, that while we are creating machine learning systems faster than we are auditing them, society is becoming increasingly vigilant in privatizing non-biased systems.\nDr. Jessica L’roe, a female professor at Middlebury, continued to emphasize the importance of context within research like Professor Brown, drawing from her own study on deforestation and afforestation in various regions of Africa and their impacts on local communities. Employing a multifaceted approach, she collected both qualitative and quantitative data, enriching numerical findings with narratives from the local populace. One notable observation she shared was the rising population of smallholder farmers around a national park in Uganda, coinciding with an increase in tree planting activities. Looking deeper into the dynamics, she uncovered that non-local landowners were primarily responsible for the tree planting initiatives, often planting non-native species. Another intriguing aspect explored was the intergenerational changes in land parcel sizes and their implications on local livelihoods. Mothers in the area were interviewed and confessed that they were prioritizing education over agricultures for the children, because they feared that there would not be available land to farm. This nuanced examination highlights the significance of considering social dynamics alongside environmental changes and data in understanding complex issues like deforestation and community resilience.\nLaura Biester was our last keynote speaker and offered a different perspective, focusing more on training models to a data set. She presented on her research on computational linguistic models of mental health. Shifting the focus towards training models with specific datasets derived from public internet forums like Twitter and Reddit, Biester explored the utilization of language, particularly the use of first-person pronouns, as a measure of decreased mental health, among individuals with depression. However, Biester also addressed the challenges associated with accessing high-quality data due to privacy concerns, emphasizing the need for more representative datasets. I had never heard of natural language processing, so I found her talk very interesting. because it also once again combined my interests of psychology and computer science. Her model discovered that natural language models do a better job of capturing emotions, and manifested depressive symptoms, while linear regression models focus more on discussion of overall mental health and medication. Through her investigation, she uncovered the potential of out-of-domain datasets to test the generalizability of trained models, paving the way for a more holistic understanding of depression, emotions, and relationships through language processing techniques.\nIn conclusion, achieving gender equality and diversity in STEM fields requires concerted efforts to challenge systemic biases and provide support for underrepresented groups. By recognizing the value of diversity and promoting inclusive representation, industries can foster innovation and creativity while ensuring equitable opportunities for all. The insights shared by various speakers, spanning disciplines from political science to linguistics, to geography highlight the multidimensional nature of the issue and all the ways that data can be explored and utilized towards a passion. Moving forward, I learned that continued education, contextualizing data and systemic reform are essential to create a more inclusive and equitable future for women both in and out of STEM professions."
  },
  {
    "objectID": "posts/Blog1/BlogPost1revised.html",
    "href": "posts/Blog1/BlogPost1revised.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAbove we are importing data from the specififed URL and then reading in that data to the variable “train” We then can use the command “.head()”, to print ot the data table in a digestable and readable way.\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis code snippet is part of a data preparation process. First we have to import LabelEncoder class from the sklearn.preprocessing module, to help use encode labels into numerical values. We then call LabelEncoderand fit it to the species columb of the tranining data. This turned our categorial labels into number digestable to the machine learning algorithm. Next, a function named prepare_data is defined to handle the preprocessing tasks. Within this function, we remove irrelevant columns such as “studyName,” “Sample Number,” and others specified in the drop() method, we filter out rows with missing values in the “Sex” column, and if there are any missing values the rows are dropped as well.pd.get_dummies()switches the categorical columns into binary encoded columns and the now prepared data set is returned.\nWe then split the data into ‘x_train’ and ‘y_train’ and call the new prepare_data function described above on our data.\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nAfter importing our necessary platforms, we then set up the cross valadation score, establishing the classifier as a random object with no parameters. CV is the amount of time we cross-validate and then return our mean scores. The provided block of code defines a function named evaluate_features that assesses the predictive performance of a set of features using cross-validation with a random forest classifier. In the ’scores array, the accuracy scores are computed adn stores before returning the mean of these scores.\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nIn the code snippet above, we define our qualitative columns and our quantative columns, and then with a nested for loop over our data we combine our columns. Finally we print out the combinations so we can get a sense of visualizing our data.\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguins.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove, the code prepares the data and then preforming a train_test spilt. We then choose which columns we want to be our qualitative data versus quantitative data and defining our classifiers.\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\nIn the code snippet above, we are now actually preforming the logicstic regression. We fit the logistic regression to our training data, highlighting which predicator column we want, then we calculate the scores.\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/Blog1/BlogPost1.html",
    "href": "posts/Blog1/BlogPost1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nSetting up cross valadation score, establishing my classifier as a random object with no paramenters. CV is the amount of time we cross-validate and then return our mean scores\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n“cols” was just for visualizing how to combine column, the training and test columns will be run on the variable “predicted colmns which includes the columns we haven’t dropped, culmen length and culmen depth. Cols above is compare all the data points across species inorder to produce the pair plot below\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguin.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove we prepared the data preparing the data and then preforming a train_test spilt\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/BlogPost1.html",
    "href": "posts/BlogPost1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nSetting up cross valadation score, establishing my classifier as a random object with no paramenters. CV is the amount of time we cross-validate and then return our mean scores\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n“cols” was just for visualizing how to combine column, the training and test columns will be run on the variable “predicted colmns which includes the columns we haven’t dropped, culmen length and culmen depth. Cols above is compare all the data points across species inorder to produce the pair plot below\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguin.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove we prepared the data preparing the data and then preforming a train_test spilt\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/WomeninDataScience.html",
    "href": "posts/WomeninDataScience.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "WOMEN IN DATA SCIENCE BLOG POST\nDespite significant advancements in feminist movements, women remain conspicuously underrepresented in fields such as computing, mathematics, and engineering, which not only affects those within the industry but also the industry itself. Diversity fosters informed decision-making and sparks creativity. However, the current representation of women in STEM careers mirrors that of the 1960s, despite increased opportunities and educational access. This disparity, particularly in engineering, stems from historical shifts in the culture surrounding computing and subsequent academic requirements that disadvantage women and marginalized groups. To address this, efforts spotlighting the achievements of women in STEM are crucial. The Women in Data Science Conference that I attended, did just this. Highlighting women working with data in a variety of fields, such as political science, geography, engineering, and computer science the conference made space to discuss the different avenues women have taken with working with data. Additionally, the conference featured an alumni panel to give insight to Middlebury female alums now working in STEM fields. I really enjoyed this section as there is always a gap between education in college and success in the career world, and I enjoyed talking to alums, both recent and no0t so recent about how they used data science to make that jump.\nEven though we got the opportunity to speak with women in data related careers, these industries overall lack female representation. When industries lack proper diversity, perspectives become skewed, hindering unbiased innovation “The United States simply can’t afford to ignore the perspectives of half the population in future engineering and technical designs” (3. Corbett and Hill, 2015). Without diversity and representation in STEM industries, “everyone misses out on the novel solutions that diverse participation brings”(8, Corbett and Hill, 2015). Sometimes for non-diverse groups, the issue isn’t generating the “wrong” or unoptimal solution, its misidentifying the problem altogether.\nRight now, 26% of the population in STEM careers are women. This is an identical representation to STEM as the 1960s, despite the increased opportunity and education of women in STEM. Engineering is the most sex-segregated profession; however, compared to other profession such as lawyers or doctors, the gap between male engineers and female is exponentially wider. In the 1960s, engineering was a new field in need of workers, so it attracted men and women alike. However, when the personal computer was developed in the 80s, it created a hobbyist culture around computers, creating a gamer subculture that became dominated by men. Given the increasingly prevalence of computers both in the home and at work, colleges added increasingly stringent requirements for the computer science major. These additional requirements disproportionately disadvantaged women and people of other marginalized groups who were entering college with less math and programming experience.\nBreaking down barriers and challenging the underrepresentation of women in these fields, can be done by events that spotlighting the achievements of women in STEM, like this panel. Its important to combat workplace hostility towards women, as they are leaving STEM based jobs at a much higher rate than men. By providing visible role models, challenging stereotypes, and building support networks, these events inspire and empower women to pursue careers in STEM. They also advocate for change by raising awareness of systemic issues such as gender bias in hiring and promotion processes. By inspiring future generations of girls and students, these events help to create a more inclusive and diverse STEM community, ultimately contributing to greater gender equality in these fields.\nThis need for diversity within disciplines, extends beyond the STEM communities. Just as the STEM industry grapples with the challenge of inclusively, Amy Yuen’s examination of the Security Council sheds light on the complexities of achieving equal representation in influential decision-making bodies, like the United Nations. These insights show the importance of addressing systemic barriers and promoting diversity to create more inclusive environments conducive to progress and equitable outcomes. Amy Yuen is political science professor, who shared with us how she utilized data and data modeling to determine if the UN security council was “democratic” and offered equal representation. She first shared some necessary background information about the council, describing how there are 5 core countries with veto power who do not move off the council, but 10 other seats in which countries campaign for based on region. Despite the obvious lack of democratic voting within the council, Yuen’s focus was on evaluating the issue of equal representation. Each country holds a rotating elected seat for a month, and Yuen measured success in terms of their output during this tenure. Interestingly, she discovered that factors such as a nation’s wealth or political policies did not significantly impact their monthly output. Instead, she found that sponsorship played a pivotal role in determining the level of output on the council. Despite some countries like Japan or Brazil frequently serving on the council, Yuen noted close to equal representation among rotating elected members.\nSimilarly, Sarah M Brown, our keynote speaker, took a broader approach in investigating equal representation. She came all the way up from the University of Rhode Island to talk to us about the ethics of machine learning and how we can create systems that accurately represent society’s composition. As we are using machine learning more and more, specifically developing machine learning for sociotechnical systems, we need to direct our focus to making machine learning more fair and evaluate the context in which we create these systems.\nHer presentation started off by defining data science as equal parts computer science, stats, and domain expertise. Contrary to common practice, she emphasized the criticality of integrating domain expertise from the outset, rather than merely consulting domain experts later in the process. Contextualizing data, she argued, is key to unlocking solutions to STEM-related challenges. Expounding on this, she presented three fundamental “keys” that have shaped her work. However, she argued that it is important to contextualize this data, and pull upon non computer science or math backgrounds to unlock the keys to solving STEM related issues. The rest of the presentation was an exploration of three “keys” that have helped to unlock her work.\nHer first key was introduced back in 2004 in her high school social studies course. Her teacher described the need to use context to decipher primary sources. Drawing parallels to data as primary sources, Brown stressed the necessity of contextual understanding in deciphering, cleaning, and implementing data effectively. Her second key highlighted the social nuances and disciplinary norms inherent in different fields. She emphasized the need to situate data and models within the framework of the relevant discipline to ensure accurate interpretation and application. As a psych major, I particularly enjoyed this part of the talk. My interest is where the intersect of these two disciplines lie. My “domain skills” are being able to connect people and social leadership with data models and computer science, which can be isolating at times. The last “key” she mentioned was to meet people where they are. She recognized the power of precedent in shaping attitudes towards innovation and change. Using examples, she illustrated the challenges in prioritizing fairness over accuracy, urging for creative solutions rather than mere mathematical refinement.\nShe briefly introduced the concept of “algorithmic reparations,” which is something I had never heard before. Brown provocatively challenged the prevailing notion that accuracy must always take precedence over fairness, highlighting the need for a shift in perspective within the data science community. From a psychological standpoint, she emphasized the imperative of creative thinking in addressing these ethical dilemmas, signaling a departure from traditional approaches grounded solely in mathematical optimization. She ended the lecture on a positive note, that while we are creating machine learning systems faster than we are auditing them, society is becoming increasingly vigilant in privatizing non-biased systems.\nDr. Jessica L’roe, a female professor at Middlebury, continued to emphasize the importance of context within research like Professor Brown, drawing from her own study on deforestation and afforestation in various regions of Africa and their impacts on local communities. Employing a multifaceted approach, she collected both qualitative and quantitative data, enriching numerical findings with narratives from the local populace. One notable observation she shared was the rising population of smallholder farmers around a national park in Uganda, coinciding with an increase in tree planting activities. Looking deeper into the dynamics, she uncovered that non-local landowners were primarily responsible for the tree planting initiatives, often planting non-native species. Another intriguing aspect explored was the intergenerational changes in land parcel sizes and their implications on local livelihoods. Mothers in the area were interviewed and confessed that they were prioritizing education over agricultures for the children, because they feared that there would not be available land to farm. This nuanced examination highlights the significance of considering social dynamics alongside environmental changes and data in understanding complex issues like deforestation and community resilience.\nLaura Biester was our last keynote speaker and offered a different perspective, focusing more on training models to a data set. She presented on her research on computational linguistic models of mental health. Shifting the focus towards training models with specific datasets derived from public internet forums like Twitter and Reddit, Biester explored the utilization of language, particularly the use of first-person pronouns, as a measure of decreased mental health, among individuals with depression. However, Biester also addressed the challenges associated with accessing high-quality data due to privacy concerns, emphasizing the need for more representative datasets. I had never heard of natural language processing, so I found her talk very interesting. because it also once again combined my interests of psychology and computer science. Her model discovered that natural language models do a better job of capturing emotions, and manifested depressive symptoms, while linear regression models focus more on discussion of overall mental health and medication. Through her investigation, she uncovered the potential of out-of-domain datasets to test the generalizability of trained models, paving the way for a more holistic understanding of depression, emotions, and relationships through language processing techniques.\nIn conclusion, achieving gender equality and diversity in STEM fields requires concerted efforts to challenge systemic biases and provide support for underrepresented groups. By recognizing the value of diversity and promoting inclusive representation, industries can foster innovation and creativity while ensuring equitable opportunities for all. The insights shared by various speakers, spanning disciplines from political science to linguistics, to geography highlight the multidimensional nature of the issue and all the ways that data can be explored and utilized towards a passion. Moving forward, I learned that continued education, contextualizing data and systemic reform are essential to create a more inclusive and equitable future for women both in and out of STEM professions."
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Second Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]