[
  {
    "objectID": "Working Blogs/Untitled-1.html",
    "href": "Working Blogs/Untitled-1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import torch\nfrom matplotlib import pyplot as plt\n\n# Generate regression data\ndef regression_data(n=100, w=torch.Tensor([-0.7, 0.5]), x_max=1):\n    x = torch.rand(n) * x_max\n    y = x * w[1] + w[0] + 0.05 * torch.rand(n)\n    return x, y\n\n# Define the empirical risk (mean squared error)\ndef empirical_risk(x, y, w, b):\n    y_pred = w * x + b\n    return torch.mean((y_pred - y)**2)\n\n# Stochastic Gradient Descent (SGD) algorithm\ndef sgd_algorithm(x, y, w_init=torch.rand(2), lr=0.1, epochs=100):\n    w = torch.tensor(w_init.clone(), requires_grad=True)\n    b = torch.tensor(torch.rand(1), requires_grad=True)\n    \n    empirical_risks = []\n    \n    for epoch in range(epochs):\n        # Shuffle indices for each epoch\n        indices = torch.randperm(x.shape[0])\n        \n        for idx in indices:\n            x_i = x[idx]\n            y_i = y[idx]\n            \n            # Compute gradients of the empirical risk (MSE) w.r.t. w and b\n            loss = (w * x_i + b - y_i)**2\n            loss.backward()\n            \n            # Update parameters using gradients\n            with torch.no_grad():\n                w -= lr * w.grad\n                b -= lr * b.grad\n                \n                # Manually zero the gradients after updating\n                w.grad.zero_()\n                b.grad.zero_()\n        \n        # Compute and store empirical risk at the end of each epoch\n        loss_epoch = empirical_risk(x, y, w, b)\n        empirical_risks.append(loss_epoch.item())\n    \n    return w, b, empirical_risks\n\n# Generate data\nx, y = regression_data()\n\n# Run SGD algorithm\nw_hat, b_hat, empirical_risks = sgd_algorithm(x, y)\n\n# Plot empirical risk evolution during a single epoch\nplt.figure(fig_size=(10, 5))\nplt.plot(empirical_risks, label='Empirical Risk')\nplt.xlabel('Iterations')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution During a Single Epoch')\nplt.legend()\nplt.show()\n\n# Plot empirical risk evolution over 100 epochs\n_, _, empirical_risks_over_epochs = sgd_algorithm(x, y, epochs=100)\n\nplt.figure(fig_size=(10, 5))\nplt.plot(empirical_risks_over_epochs, label='Empirical Risk')\nplt.xlabel('Epochs')\nplt.ylabel('Empirical Risk')\nplt.title('Empirical Risk Evolution Over 100 Epochs')\nplt.legend()\nplt.show()\n\n/var/folders/10/9wfcg10s3yz16fgwcvljkq340000gn/T/ipykernel_7636/925773991.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  w = torch.tensor(w_init.clone(), requires_grad=True)\n/var/folders/10/9wfcg10s3yz16fgwcvljkq340000gn/T/ipykernel_7636/925773991.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  b = torch.tensor(torch.rand(1), requires_grad=True)\n\n\nRuntimeError: grad can be implicitly created only for scalar outputs"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Blog3/WomeninDataScience.html",
    "href": "posts/Blog3/WomeninDataScience.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "WOMEN IN DATA SCIENCE BLOG POST\nDespite significant advancements in feminist movements, women remain conspicuously underrepresented in fields such as computing, mathematics, and engineering, which not only affects those within the industry but also the industry itself. Diversity fosters informed decision-making and sparks creativity. However, the current representation of women in STEM careers mirrors that of the 1960s, despite increased opportunities and educational access. This disparity, particularly in engineering, stems from historical shifts in the culture surrounding computing and subsequent academic requirements that disadvantage women and marginalized groups. To address this, efforts spotlighting the achievements of women in STEM are crucial. The Women in Data Science Conference that I attended, did just this. Highlighting women working with data in a variety of fields, such as political science, geography, engineering, and computer science the conference made space to discuss the different avenues women have taken with working with data. Additionally, the conference featured an alumni panel to give insight to Middlebury female alums now working in STEM fields. I really enjoyed this section as there is always a gap between education in college and success in the career world, and I enjoyed talking to alums, both recent and no0t so recent about how they used data science to make that jump.\nEven though we got the opportunity to speak with women in data related careers, these industries overall lack female representation. When industries lack proper diversity, perspectives become skewed, hindering unbiased innovation “The United States simply can’t afford to ignore the perspectives of half the population in future engineering and technical designs” (3. Corbett and Hill, 2015). Without diversity and representation in STEM industries, “everyone misses out on the novel solutions that diverse participation brings”(8, Corbett and Hill, 2015). Sometimes for non-diverse groups, the issue isn’t generating the “wrong” or unoptimal solution, its misidentifying the problem altogether.\nRight now, 26% of the population in STEM careers are women. This is an identical representation to STEM as the 1960s, despite the increased opportunity and education of women in STEM. Engineering is the most sex-segregated profession; however, compared to other profession such as lawyers or doctors, the gap between male engineers and female is exponentially wider. In the 1960s, engineering was a new field in need of workers, so it attracted men and women alike. However, when the personal computer was developed in the 80s, it created a hobbyist culture around computers, creating a gamer subculture that became dominated by men. Given the increasingly prevalence of computers both in the home and at work, colleges added increasingly stringent requirements for the computer science major. These additional requirements disproportionately disadvantaged women and people of other marginalized groups who were entering college with less math and programming experience.\nBreaking down barriers and challenging the underrepresentation of women in these fields, can be done by events that spotlighting the achievements of women in STEM, like this panel. Its important to combat workplace hostility towards women, as they are leaving STEM based jobs at a much higher rate than men. By providing visible role models, challenging stereotypes, and building support networks, these events inspire and empower women to pursue careers in STEM. They also advocate for change by raising awareness of systemic issues such as gender bias in hiring and promotion processes. By inspiring future generations of girls and students, these events help to create a more inclusive and diverse STEM community, ultimately contributing to greater gender equality in these fields.\nThis need for diversity within disciplines, extends beyond the STEM communities. Just as the STEM industry grapples with the challenge of inclusively, Amy Yuen’s examination of the Security Council sheds light on the complexities of achieving equal representation in influential decision-making bodies, like the United Nations. These insights show the importance of addressing systemic barriers and promoting diversity to create more inclusive environments conducive to progress and equitable outcomes. Amy Yuen is political science professor, who shared with us how she utilized data and data modeling to determine if the UN security council was “democratic” and offered equal representation. She first shared some necessary background information about the council, describing how there are 5 core countries with veto power who do not move off the council, but 10 other seats in which countries campaign for based on region. Despite the obvious lack of democratic voting within the council, Yuen’s focus was on evaluating the issue of equal representation. Each country holds a rotating elected seat for a month, and Yuen measured success in terms of their output during this tenure. Interestingly, she discovered that factors such as a nation’s wealth or political policies did not significantly impact their monthly output. Instead, she found that sponsorship played a pivotal role in determining the level of output on the council. Despite some countries like Japan or Brazil frequently serving on the council, Yuen noted close to equal representation among rotating elected members.\nSimilarly, Sarah M Brown, our keynote speaker, took a broader approach in investigating equal representation. She came all the way up from the University of Rhode Island to talk to us about the ethics of machine learning and how we can create systems that accurately represent society’s composition. As we are using machine learning more and more, specifically developing machine learning for sociotechnical systems, we need to direct our focus to making machine learning more fair and evaluate the context in which we create these systems.\nHer presentation started off by defining data science as equal parts computer science, stats, and domain expertise. Contrary to common practice, she emphasized the criticality of integrating domain expertise from the outset, rather than merely consulting domain experts later in the process. Contextualizing data, she argued, is key to unlocking solutions to STEM-related challenges. Expounding on this, she presented three fundamental “keys” that have shaped her work. However, she argued that it is important to contextualize this data, and pull upon non computer science or math backgrounds to unlock the keys to solving STEM related issues. The rest of the presentation was an exploration of three “keys” that have helped to unlock her work.\nHer first key was introduced back in 2004 in her high school social studies course. Her teacher described the need to use context to decipher primary sources. Drawing parallels to data as primary sources, Brown stressed the necessity of contextual understanding in deciphering, cleaning, and implementing data effectively. Her second key highlighted the social nuances and disciplinary norms inherent in different fields. She emphasized the need to situate data and models within the framework of the relevant discipline to ensure accurate interpretation and application. As a psych major, I particularly enjoyed this part of the talk. My interest is where the intersect of these two disciplines lie. My “domain skills” are being able to connect people and social leadership with data models and computer science, which can be isolating at times. The last “key” she mentioned was to meet people where they are. She recognized the power of precedent in shaping attitudes towards innovation and change. Using examples, she illustrated the challenges in prioritizing fairness over accuracy, urging for creative solutions rather than mere mathematical refinement.\nShe briefly introduced the concept of “algorithmic reparations,” which is something I had never heard before. Brown provocatively challenged the prevailing notion that accuracy must always take precedence over fairness, highlighting the need for a shift in perspective within the data science community. From a psychological standpoint, she emphasized the imperative of creative thinking in addressing these ethical dilemmas, signaling a departure from traditional approaches grounded solely in mathematical optimization. She ended the lecture on a positive note, that while we are creating machine learning systems faster than we are auditing them, society is becoming increasingly vigilant in privatizing non-biased systems.\nDr. Jessica L’roe, a female professor at Middlebury, continued to emphasize the importance of context within research like Professor Brown, drawing from her own study on deforestation and afforestation in various regions of Africa and their impacts on local communities. Employing a multifaceted approach, she collected both qualitative and quantitative data, enriching numerical findings with narratives from the local populace. One notable observation she shared was the rising population of smallholder farmers around a national park in Uganda, coinciding with an increase in tree planting activities. Looking deeper into the dynamics, she uncovered that non-local landowners were primarily responsible for the tree planting initiatives, often planting non-native species. Another intriguing aspect explored was the intergenerational changes in land parcel sizes and their implications on local livelihoods. Mothers in the area were interviewed and confessed that they were prioritizing education over agricultures for the children, because they feared that there would not be available land to farm. This nuanced examination highlights the significance of considering social dynamics alongside environmental changes and data in understanding complex issues like deforestation and community resilience.\nLaura Biester was our last keynote speaker and offered a different perspective, focusing more on training models to a data set. She presented on her research on computational linguistic models of mental health. Shifting the focus towards training models with specific datasets derived from public internet forums like Twitter and Reddit, Biester explored the utilization of language, particularly the use of first-person pronouns, as a measure of decreased mental health, among individuals with depression. However, Biester also addressed the challenges associated with accessing high-quality data due to privacy concerns, emphasizing the need for more representative datasets. I had never heard of natural language processing, so I found her talk very interesting. because it also once again combined my interests of psychology and computer science. Her model discovered that natural language models do a better job of capturing emotions, and manifested depressive symptoms, while linear regression models focus more on discussion of overall mental health and medication. Through her investigation, she uncovered the potential of out-of-domain datasets to test the generalizability of trained models, paving the way for a more holistic understanding of depression, emotions, and relationships through language processing techniques.\nIn conclusion, achieving gender equality and diversity in STEM fields requires concerted efforts to challenge systemic biases and provide support for underrepresented groups. By recognizing the value of diversity and promoting inclusive representation, industries can foster innovation and creativity while ensuring equitable opportunities for all. The insights shared by various speakers, spanning disciplines from political science to linguistics, to geography highlight the multidimensional nature of the issue and all the ways that data can be explored and utilized towards a passion. Moving forward, I learned that continued education, contextualizing data and systemic reform are essential to create a more inclusive and equitable future for women both in and out of STEM professions."
  },
  {
    "objectID": "posts/Blog1/BlogPost1.html",
    "href": "posts/Blog1/BlogPost1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nSetting up cross valadation score, establishing my classifier as a random object with no paramenters. CV is the amount of time we cross-validate and then return our mean scores\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n“cols” was just for visualizing how to combine column, the training and test columns will be run on the variable “predicted colmns which includes the columns we haven’t dropped, culmen length and culmen depth. Cols above is compare all the data points across species inorder to produce the pair plot below\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguin.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove we prepared the data preparing the data and then preforming a train_test spilt\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/Blog1/BlogPost1revised.html",
    "href": "posts/Blog1/BlogPost1revised.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nAbove we are importing data from the specififed URL and then reading in that data to the variable “train” We then can use the command “.head()”, to print ot the data table in a digestable and readable way.\n\n#Data preparation copied from assignment\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis code snippet is part of a data preparation process. First we have to import LabelEncoder class from the sklearn.preprocessing module, to help use encode labels into numerical values. We then call LabelEncoderand fit it to the species columb of the tranining data. This turned our categorial labels into number digestable to the machine learning algorithm. Next, a function named prepare_data is defined to handle the preprocessing tasks. Within this function, we remove irrelevant columns such as “studyName,” “Sample Number,” and others specified in the drop() method, we filter out rows with missing values in the “Sex” column, and if there are any missing values the rows are dropped as well.pd.get_dummies()switches the categorical columns into binary encoded columns and the now prepared data set is returned.\nWe then split the data into ‘x_train’ and ‘y_train’ and call the new prepare_data function described above on our data.\n\n# important import statements to help visualize the data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Setting up data evualtions\ndef evaluate_features(features, X, y):\n    clf = RandomForestClassifier()  \n    scores = cross_val_score(clf, X[features], y, cv=5, scoring='accuracy')\n    return np.mean(scores)\n\nAfter importing our necessary platforms, we then set up the cross valadation score, establishing the classifier as a random object with no parameters. CV is the amount of time we cross-validate and then return our mean scores. The provided block of code defines a function named evaluate_features that assesses the predictive performance of a set of features using cross-validation with a random forest classifier. In the ’scores array, the accuracy scores are computed adn stores before returning the mean of these scores.\n\n# Combination copied from assignemnt\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nIn the code snippet above, we define our qualitative columns and our quantative columns, and then with a nested for loop over our data we combine our columns. Finally we print out the combinations so we can get a sense of visualizing our data.\n\n# Combine quantitative features with the target variable for visualization\npenguins_explore = train[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']]\n\n# Plot pairplot\nsns.pairplot(penguins_explore, hue='Species')\nplt.title('Pairplot of Penguin Features')\n\nplt.xlabel('Feature Values')  # X-axis label\nplt.ylabel('Feature Values')  # Y-axis label\nplt.legend(title='Species')  # Legend title\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n\n\n\n\n\n\n\n\n\nSeaborn’s pairplot feature shows us different features across different penguin species. It is a good tool to explore relationships between multiple variables and to visualize the data to make comparisons so we can see what characteristics contribute to distunguishing between species. In this case, the pairplot is applied to the penguins_explore dataframe, which includes the species of penguins and quantitative features such as culmen length, culmen depth, flipper length, and body mass.\nEach scatterplot in the grid represents the relationship between two quantitative variables, with each point representing a penguin. The histograms along the diagonal show the distribution of each individual variable. The different colors in the graph represent the different species of penguin.\n\n# Plot swarm plot\nplt.figure(figsize=(10, 6))\nsns.swarmplot(x='Species', y='Culmen Length (mm)', data=train)\nplt.title('Distribution of Culmen Length by Species')\nplt.xlabel('Species')\nplt.ylabel('Culmen Length (mm)')\nplt.show()\n\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lenoxherman/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nAbove is a swarmplot outlining the how individual data points are distributed across different categories. This plot shows the distribution of culmen length across different species. This graph can help visualize the density of data point and determine the frequency and provide context for data points like the mean or mode.\n\nsummary_table = train.groupby('Species').agg({'Culmen Length (mm)': 'mean',\n                                              'Culmen Depth (mm)': 'mean',\n                                              'Flipper Length (mm)': 'mean',\n                                              'Body Mass (g)': 'mean'}).reset_index()\nsummary_table\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n38.970588\n18.409244\n190.084034\n3718.487395\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n48.826316\n18.366667\n196.000000\n3743.421053\n\n\n2\nGentoo penguin (Pygoscelis papua)\n47.073196\n14.914433\n216.752577\n5039.948454\n\n\n\n\n\n\n\nThe summary table shows us mean of each category for each specie so we can easily compare quantitative data. This table can help us to compare specific features across different species of penguins.\n\n#Multi-way classification, copied from assginment\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\n\n\n#Data preparation \nX_train, y_train = prepare_data(train)\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n\n# Feature selection\nquantitative_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)'] \nqualitative_feature = ['Sex']\n\n# Train and test models\nclassifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Support Vector Machine\": SVC()\n}\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columnMu\n# as suggested in the code block above\n# cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\n  \n\nAbove, the code prepares the data and then preforming a train_test spilt. We then choose which columns we want to be our qualitative data versus quantitative data and defining our classifiers.\n\n#Test- train split \nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\n\n\npredictor_cols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\ntarget_col = \"Species\"\n\nAbove we chose our features that we are going to apply to the test data. For this model we want to correctly predict specie 100% of the time based on culmen length and depth.\n\nLR = LogisticRegression()\nLR.fit(X_train[predictor_cols], y_train)\nLR.score(X_train[predictor_cols], y_train)\nLR.coef_\n\narray([[-0.87699872,  1.93957404],\n       [ 0.2923175 ,  0.3240132 ],\n       [ 0.58468122, -2.26358723]])\n\n\nIn the code snippet above, we are now actually preforming the logicstic regression. We fit the logistic regression to our training data, highlighting which predicator column we want, then we calculate the scores.\n\n# Column combinations \nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\nAgain, above demonstrates differnt combinations of features, even though we don’t apply these to our test data.\n\nfrom statistics import mean\n\n\n# LR.fit(X_test, y_test)\n# LR.score(X_test, y_test)\npreds = LR.predict(X_test[predictor_cols])\nprint(y_test == preds)\n\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nAbove you can see that we were able to predict specie based on culmen length and depth with 100% accuracy!"
  },
  {
    "objectID": "posts/Blog 4/Blog5.html",
    "href": "posts/Blog 4/Blog5.html",
    "title": "Abstract",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nIn this blog post, I will implement perceptron on linearly separable data, non-linearly separable data, and 5d data, as well as visualizations and comments to describe what each code block is executing. That data I wil be using is taken form the class notes and then modified to fit the criteria I listed above. I will learn and test how the perceptron algorithm works by evaluating its performance on different data forms. Some data points and visualizations are taken from the class notes so credit goes to Professor Phil!\n(Here is the link to my perceptron file, https://vscode.dev/github/lenoxherman/lenoxherman.github.io/blob/main/posts/Blog%204/perceptron.py )\n\n\nImplementing Perceptron\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(12989)\n\ndef perceptron_data(n_points = 100, noise = 0.23, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 100, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere is example code taken from a warmup, that shows linear separable data. I will then run my implemented perceptron on this data. Look into my perceptron python file to find where I implemented my own grad and step functions.\n\nfrom perceptron import Perceptron,perceptron_data, PerceptronOptimizer\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nTo test out my implementation of perceptron, I ran a minimal training loop using the data from above.\n\ny\np.predict(X)\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nThe graph above shows the steps of my implemented algorithm as the loss approaches 0. After a little over 1400, the loss is at 0!\n\ndef accuracy(X, y):\n\n    predictions = p.predict(X)\n    \n    predictions = 2*predictions - 1\n    \n    preds = (predictions == y).float()\n    accuracy = torch.mean(preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\naccuracy(X, y)\n\nAccuracy: 1.0\n\n\nThe function above test the accuracy of our model and as seen above we have 100% accuracy!\n\n\nVisualizations\nWe use the data from minimal training loop to show the line that linear seperates the data.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\nplt.title(\"Final Decision Boundary\")\n\nText(0.5, 1.0, 'Final Decision Boundary')\n\n\n\n\n\n\n\n\n\nThis code was taken from the notes, and draws a line between the two sets of points.\n\nImplementing Perceptron on Data that is not Linearly Separable\nI made a new plot below that has overlapping points, so the data is no longer linear separable\n\nX, y = perceptron_data(n_points = 50, noise = 0.7)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nHere the line seperating the two data sets, isn’t very accurate and doesn’t do a good job splitting up the data\n\nX, y = perceptron_data(n_points = 50, noise = 0.7)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\n\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe can from the visualizations above, that the data is not linearly separable, and its impossible to draw a line that splits the two datasets\n\nfrom perceptron import Perceptron,perceptron_data, PerceptronOptimizer\nimport torch\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\n\n\nloss = 1.0\nmax_iterations = 1000\n\nn = X.size()[0]\n\nwhile loss &gt; 0  and max_iterations &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[i,:].squeeze()\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iterations -= 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAbove is a graph, once again mapping the loss as it approaches 0. This time after 1000 iterations, the loss ossilates a lot, and never converges at 0.\n\n\nImplementing Perceptron on Data that is not 2D\nThis code will run perceptron on 5D data.\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,5))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.35)\n\n\nX, y = perceptron_data()\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec = []\nmax_iterations = 1000\n\nloss = 1.0\n\nn = X.size()[0]\n\nwhile loss &gt; 0  and max_iterations &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[i,:].squeeze()\n    y_i = y[i]\n\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    max_iterations -= 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nAfter 1000 iterations, the loss is almost at 0!\n\naccuracy(X, y)\n\nAccuracy: 0.9866666793823242\n\n\nWith 5D data, after 1000 iterations the loss is almost at 0 and our accuracy is at 98.6%, which is almost 1000, very similar to when we implemented perceptron on not linearly separable data.\n\n\n\nConclusion\nIn this blog, I coded my own version of the perceptron algorithm, which is provided in the attached Python file. Testing this implementation on linearly separable data revealed that the loss approached 0, indicating successful classification. However, when applying the algorithm to non-linearly separable data, it became evident that convergence might never occur, potentially causing the algorithm to run indefinitely. To address this issue, I introduced a maximum iteration limit to prevent infinite execution. Despite this limitation, the accuracy achieved was nearly 100%. Nevertheless, it’s important to note that the perceptron algorithm may not achieve perfect separation between the two datasets. Furthermore, I extended the application of the perceptron to 5D data, achieving a loss of 0 once again. Overall, my implementation of the perceptron algorithm proved successful across all three types of datasets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Abstract\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Working Blogs/Warmup3:28.html",
    "href": "Working Blogs/Warmup3:28.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "def mystery_fun(a, epsilon, alpha, max_iterations = 100):\n    # Initial guess\n    x = a \n    xPrime = 0\n    j = 0\n\n    # Set iteration counter and maximum number of steps\n    \n    while abs(xPrime - x) &gt; epsilon:\n        # Calculate the value of f(x) using the given formula\n        \n        # If the absolute difference between f(x) and 0 is less than tolerance, break\n        if j &gt; max_iterations:\n            break\n        loss = x - (a / x)\n        xPrime= x\n        x = x - alpha * loss\n        j += 1\n    \n    return x\n\n# Testing the function\n# Test 1: Setting of a for which the function returns a real number very close to the exact value of sqrt(a)\nresult1 = mystery_fun(a=9, epsilon=1e-8, alpha=0.2)\nprint(\"Result 1:\", result1)\n\nresult2 = mystery_fun(a=9, epsilon=1e-8, alpha=2)\nprint(\"Result 2:\", result2)\n\nresult3 = mystery_fun(a=9, epsilon=1e-8, alpha=.001)\nprint(\"Result 3:\", result3)\n\n\nResult 1: 3.000000013459385\nResult 2: -76.62213126914914\nResult 3: 8.235542835355941"
  }
]